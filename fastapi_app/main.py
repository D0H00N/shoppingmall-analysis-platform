from fastapi import FastAPI, Request, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.responses import RedirectResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Dict, Any
from collections import Counter
import pymysql
from datetime import datetime, timedelta, date
from typing import Optional
from fastapi.middleware.cors import CORSMiddleware
import asyncio
import json
import aioredis
import re
import aiohttp
import aiomysql
from fastapi.responses import JSONResponse  # âœ… ì¶”ê°€
from aiomysql.cursors import DictCursor  # âœ… ì˜¬ë°”ë¥¸ import
import calendar

app = FastAPI()

# ì •ì  íŒŒì¼ ì„œë¹™ (ì´ë¯¸ì§€, CSS ë“±)
app.mount("/static", StaticFiles(directory="static"), name="static")

# í…œí”Œë¦¿ ì„¤ì • (HTML íŒŒì¼ ìœ„ì¹˜)
templates = Jinja2Templates(directory="templates")

# MySQL ì—°ê²° ì •ë³´
DB_CONFIG = {
    "host": "15.152.242.221",
    "user": "admin",
    "password": "Admin@1234",
    "database": "musinsa_pd_rv",  # âœ… ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
    "cursorclass": pymysql.cursors.DictCursor,
}

# CORS ì„¤ì • (í•„ìˆ˜)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš© (ë³´ì•ˆ ì„¤ì • í•„ìš”)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class DateFilter(BaseModel):
    year: Optional[int] = None  # âœ… Optional[int]ìœ¼ë¡œ ë³€ê²½
    month: Optional[int] = None
    day: Optional[int] = None


# í…Œì´ë¸”ëª… ìë™ ì„¤ì •
TODAY_DATE = datetime.today().strftime("%Y%m%d")
TABLE_NAME = f"today_reviews_{TODAY_DATE}_analysis"


### ğŸ”¹ MySQL ì—°ê²° í•¨ìˆ˜
def get_mysql_connection():
    try:
        return pymysql.connect(**DB_CONFIG)
    except pymysql.MySQLError as e:
        print(f"MySQL Connection Error: {e}")
        return None

# ê¸°ë³¸ í˜ì´ì§€ ë Œë”ë§ (index.html)
@app.get("/")
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})
#--------------------------------------------------------#
# ì‚¬ì´ì¦ˆ ë¶„ì„ í˜ì´ì§€ ë Œë”ë§ (size_analysis.html)
@app.get("/size_analysis.html")
async def size_analysis_page(request: Request):
    return templates.TemplateResponse("size_analysis.html", {"request": request})

# âœ… review_size ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_review_size(size):
    size = size.strip()

    # 1ï¸âƒ£ ê´„í˜¸ ì•ˆ ìˆ«ì ì¤‘ì—ì„œ ê°€ì¥ í° ê°’ ì¶”ì¶œ (ex: (19)Goofy Black Â· 225 êµ¬ë§¤ -> 225)
    bracket_match = re.findall(r'\((\d+)\)', size)
    if bracket_match:
        return max(bracket_match, key=int)  # ê°€ì¥ í° ìˆ«ìë¥¼ ë°˜í™˜ (ìƒ‰ìƒ ì½”ë“œë³´ë‹¤ ì‚¬ì´ì¦ˆê°€ ë” í´ ê°€ëŠ¥ì„± ë†’ìŒ)

    # 2ï¸âƒ£ '~' í¬í•¨ëœ ê²½ìš° â†’ ì²« ë²ˆì§¸ ìˆ«ì ì¶”ì¶œ (ex: 230~240 -> 230)
    range_match = re.search(r'(\d+)~\d+', size)
    if range_match:
        return range_match.group(1)

    # 3ï¸âƒ£ ìˆ«ì + ì†Œìˆ˜ì  ìˆëŠ” ê²½ìš° (ex: 40.5 ë“±)
    num_match = re.search(r'\b\d+(\.\d+)?\b', size)
    if num_match:
        return num_match.group(0)

    # 4ï¸âƒ£ ì•ŒíŒŒë²³ ì‚¬ì´ì¦ˆ ì¶”ì¶œ (M, L, S, XS, XL, XXL ë“±)
    alpha_match = re.search(r'\b(M|L|S|XS|XL|XXL)\b', size, re.IGNORECASE)
    if alpha_match:
        return alpha_match.group(0).upper()

    # 5ï¸âƒ£ ì¶”ê°€ íŒ¨í„´ ì²˜ë¦¬: êµ¬ë§¤ ë¬¸êµ¬ í¬í•¨ëœ ê²½ìš° ìˆ«ì ì¶”ì¶œ (ex: "DARK GREY(401) Â· 225 êµ¬ë§¤" â†’ "225")
    purchase_match = re.search(r'(\d{2,3})\s*êµ¬ë§¤', size)
    if purchase_match:
        return purchase_match.group(1)
    
    # 6ï¸âƒ£ í•œê¸€ê³¼ ìˆ«ìê°€ ì„ì—¬ ìˆëŠ” ê²½ìš° â†’ ìˆ«ìë§Œ ì¶”ì¶œ (ex: "ë¸”ë™ 225" â†’ "225")
    mixed_match = re.findall(r'\d{2,3}', size)
    if mixed_match:
        return max(mixed_match, key=int)  # ê°€ì¥ í° ìˆ«ìë¥¼ ë°˜í™˜

    # 6ï¸âƒ£ ìˆ«ìë„ ì—†ê³  ì•ŒíŒŒë²³ ì‚¬ì´ì¦ˆë„ ì—†ìœ¼ë©´ "-"
    return "-"

# âœ… 1. ì¹´í…Œê³ ë¦¬ë³„ ë¶€ì • ë¹„ìœ¨ 50% ì´ìƒ ë¦¬ìŠ¤íŠ¸
def find_most_negative_categories(results):
    category_negatives = {}

    for row in results:
        category = row.get("category")
        negative_ratio = row["negative_ratio"]

        if negative_ratio >= 50:
            if category not in category_negatives:
                category_negatives[category] = []
            category_negatives[category].append(row)

    return [item for sublist in category_negatives.values() for item in sublist]

# âœ… 2. ë¸Œëœë“œë³„ ë¶€ì • ë¹„ìœ¨ 50% ì´ìƒ ë¦¬ìŠ¤íŠ¸
def find_most_negative_brands(results):
    brand_negatives = {}

    for row in results:
        brand = row.get("brand")
        negative_ratio = row["negative_ratio"]

        if negative_ratio >= 50:
            if brand not in brand_negatives:
                brand_negatives[brand] = []
            brand_negatives[brand].append(row)

    return [item for sublist in brand_negatives.values() for item in sublist]

# âœ… 3. ì „ì²´ì—ì„œ ë¶€ì • ë¹„ìœ¨ì´ ê°€ì¥ ë†’ì€ ì‚¬ì´ì¦ˆ ë¦¬ìŠ¤íŠ¸
def find_most_negative_sizes(results):
    if not results:
        return []

    max_negative = max(row["negative_ratio"] for row in results)

        # if max_negative >= 50:
    return [row for row in results if row["negative_ratio"] == max_negative]
        # return []

# ì‚¬ì´ì¦ˆ ë¶„ì„ API
@app.get("/api/size-analysis")
async def size_analysis() -> Dict[str, Any]:  # íƒ€ì…ì„ Dict[str, Any]ë¡œ ìˆ˜ì •
    try:
        conn = get_mysql_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)

        query = """
         SELECT 
            s.product_id, s.review_size, s.total_reviews, s.avg_rating, s.negative_ratio, 
            p.product_name, p.brand, p.category, p.gender, p.price, p.image_url,
            -- âœ… ì¤‘ë¦½(0) ë˜ëŠ” ë¶€ì •(1) ë¦¬ë·°ë§Œ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 5ê°œ)
            (SELECT GROUP_CONCAT(DISTINCT r.sentence ORDER BY r.review_id DESC SEPARATOR ' || ') 
            FROM review_analysis r 
            WHERE r.product_id = s.product_id 
            AND r.category LIKE '%ì‚¬ì´ì¦ˆ%'  
            AND r.sentiment IN (0,1)  
            LIMIT 5) AS reviews
        FROM size_analysis_view s
        JOIN products p ON s.product_id = p.product_id
        GROUP BY s.product_id, s.review_size, s.total_reviews, s.avg_rating, s.negative_ratio, 
                p.product_name, p.brand, p.category, p.gender, p.price, p.image_url;
        """

        cursor.execute(query)
        results = cursor.fetchall()

        if not results:
            raise HTTPException(status_code=404, detail="No data found in size_analysis_view")
 
         # âœ… review_size ì „ì²˜ë¦¬ ì ìš©
        for row in results:
            row["review_size"] = preprocess_review_size(row["review_size"])


        # ğŸ”¹ ìµœì¢… ê²°ê³¼ ì €ì¥
        most_negative_categories = find_most_negative_categories(results)
        most_negative_brands = find_most_negative_brands(results)
        most_negative_sizes = find_most_negative_sizes(results)
            
        # ê°€ì¥ ë§ì´ ë“±ì¥í•œ ì‚¬ì´ì¦ˆ ì°¾ê¸° 
        def most_frequent_size(size_list):
            if not size_list:
                return "N/A"
            size_counter = Counter(row["review_size"] for row in size_list)
            return size_counter.most_common(1)[0][0]  # e.g., "260"

        # âœ… JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •
        response_data = {
            "size_analysis": results,
            "most_negative_categories": most_negative_categories,
            "most_negative_brands": most_negative_brands,
            "most_negative_sizes": most_negative_sizes,
            "summary_info": {
                "category_negative_count": len(most_negative_categories),
                "most_negative_count": most_frequent_size(most_negative_sizes),
                "brand_negative_count": len(most_negative_brands),
                "most_frequent_most_negative": most_frequent_size(most_negative_sizes),
            }
        }

        cursor.close()
        conn.close()

        return response_data

    except pymysql.err.ProgrammingError as e:
        raise HTTPException(status_code=500, detail=f"MySQL Programming Error: {str(e)}")

    except pymysql.err.OperationalError as e:
        raise HTTPException(status_code=500, detail=f"MySQL Connection Error: {str(e)}")

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Unexpected Error: {str(e)}")


# ë¸Œëœë“œ ë¶„ì„ í‰ê°€ API
@app.get("/api/brand-analysis/{brand}")
async def brand_analysis(brand: str) -> Dict[str, Any]:
    try:
        conn = get_mysql_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)

        print(f"ğŸ” ë¸Œëœë“œ ë¶„ì„ ìš”ì²­: {brand}")

        # âœ… í•´ë‹¹ ë¸Œëœë“œì˜ ëª¨ë“  ì œí’ˆ ID ì¡°íšŒ
        product_query = """
        SELECT product_id, product_name, category
        FROM products
        WHERE brand = %s
        """
        cursor.execute(product_query, (brand,))
        products = cursor.fetchall()

        if not products:
            return {"message": "â— No products found for this brand", "topics": {}, "raw_data": []}

        product_ids = [p["product_id"] for p in products]

        # âœ… ê°ì„± ë¶„ì„ ë°ì´í„° ì¡°íšŒ
        query = """
        SELECT category, sentiment 
        FROM review_analysis 
        WHERE product_id IN (%s)
        """ % ','.join(['%s'] * len(product_ids))  # IN ì ˆì„ ë™ì ìœ¼ë¡œ ìƒì„±
        cursor.execute(query, tuple(product_ids))
        results = cursor.fetchall()

        # print(f"ğŸŸ¡ ê°ì„± ë¶„ì„ ë°ì´í„°: {results}")  

        if not results:
            return {"message": "â— No sentiment data found for this brand", "topics": {}, "raw_data": []}


        # âœ… ê°ì„± ë¶„ì„ ë°ì´í„° ê°€ê³µ
        topic_scores = {
            "ë‚´êµ¬ì„± ë° í’ˆì§ˆ": [],
            "ë””ìì¸": [],
            "ì‚¬ì´ì¦ˆ": [],
            "ê°€ì„±ë¹„": [],
            "ì°©ìš©ê°": [],
            "ë°°ì†¡ ë° í¬ì¥ ë° ì‘ëŒ€": []
        }
        topic_counts = {key: 0 for key in topic_scores}  

        for review in results:
            category = review["category"]
            sentiment = review["sentiment"]

            if category and sentiment is not None:
                categories = category.split(",")  
                for cat in categories:
                    cat = cat.strip()
                    if cat in topic_scores:
                        topic_scores[cat].append(sentiment)
                        topic_counts[cat] += 1  

        # âœ… í† í”½ë³„ ë§Œì¡±ë„ ê³„ì‚°
        topic_aggregated = {
            topic: {
                "score": round((sum(scores) / len(scores)) * 50, 2) if scores else 0,  # 0~100 ë³€í™˜
                "count": topic_counts[topic]
            }
            for topic, scores in topic_scores.items()
        }


        # âœ… ë¸Œëœë“œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        review_count = len(results)  # ì „ì²´ ë¦¬ë·° ê°œìˆ˜
        avg_sentiment = sum(r["sentiment"] for r in results) / review_count * 50 if review_count > 0 else 0
        # avg_rating = sum(r["sentiment"] for r in results) / review_count if review_count > 0 else 0
        # past_reviews = review_count - recent_reviews  # ê³¼ê±° ë¦¬ë·° ê°œìˆ˜

        # âœ… ë¦¬ë·° ê°œìˆ˜ ê¸°ì¤€ ì ìš©
        if review_count < 30:
            review_weight = 0.7  # ë¦¬ë·° ê°œìˆ˜ ì ìœ¼ë©´ ì‹ ë¢°ë„ ë‚®ìŒ(ê°ì  í¬ê²Œ)
        elif review_count < 100:
            review_weight = 0.85
        elif review_count < 300:
            review_weight = 0.95
        else:
            review_weight = 1.0  # ë¦¬ë·° ê°œìˆ˜ê°€ ì¶©ë¶„í•˜ë©´ ê°ì  ì—†ìŒ

        # âœ… ìµœê·¼ ë¦¬ë·° ë¹„ìœ¨ ì ìš©
        recent_reviews = sum(1 for r in results if r["sentiment"] >= 0.7)  # ìµœê·¼ ê¸ì • ë¦¬ë·° ê°œìˆ˜
        recent_ratio = recent_reviews / max(review_count, 1)  

        if recent_ratio < 0.3:
            recent_weight = 0.85  # ê¸ì • ë¦¬ë·° ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ ê°ì 
        elif recent_ratio < 0.5:
            recent_weight = 0.95
        else:
            recent_weight = 1.0  # ìµœê·¼ ê¸ì • ë¦¬ë·° ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ ê°ì  ì—†ìŒ

        # âœ… íŠ¹ì • í† í”½ ì ìˆ˜ ì ìš©
        min_topic_score = min(topic["score"] for topic in topic_aggregated.values() if topic["count"] > 0)

        if min_topic_score < 60:
            topic_weight = 0.8
        elif min_topic_score < 75:
            topic_weight = 0.9
        else:
            topic_weight = 1.0  # íŠ¹ì • í† í”½ì˜ ì ìˆ˜ê°€ ë†’ìœ¼ë©´ ê°ì  ì—†ìŒ

        # # âœ… í‰ì  ì ìš©
        # if avg_rating and avg_rating <= 4.7:overall_score *= 0.97
        # elif avg_rating and avg_rating <= 4.6:overall_score *= 0.95

        # âœ… ìµœëŒ€ 100ì  ì œí•œ
        overall_score = avg_sentiment * review_weight * recent_weight * topic_weight
        overall_score = min(100, round(overall_score, 2))
        
        def get_brand_rating(score: float) -> str:
            """ë¸Œëœë“œ ì¢…í•© í‰ê°€ ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
            if score < 60:
                return "ë¯¸í¡"
            elif score < 80:
                return "ë³´í†µ"
            else:
                return "ì¢‹ìŒ"

        # âœ… ë¶€ì •(1) ê°œìˆ˜ ì¡°íšŒ
        negative_query = """
        SELECT category, COUNT(*) as count
        FROM review_analysis
        WHERE product_id IN (%s) AND sentiment = 1
        GROUP BY category
        """ % ','.join(['%s'] * len(product_ids))

        cursor.execute(negative_query, tuple(product_ids))
        negative_results = cursor.fetchall()

        # âœ… ì¤‘ë¦½(0) ê°œìˆ˜ ì¡°íšŒ
        neutral_query = """
        SELECT category, COUNT(*) as count
        FROM review_analysis
        WHERE product_id IN (%s) AND sentiment = 0
        GROUP BY category
        """ % ','.join(['%s'] * len(product_ids))

        cursor.execute(neutral_query, tuple(product_ids))
        neutral_results = cursor.fetchall()

        # âœ… ê¸ì •(2) ê°œìˆ˜ ì¡°íšŒ
        positive_query = """
        SELECT category, COUNT(*) as count
        FROM review_analysis
        WHERE product_id IN (%s) AND sentiment = 2
        GROUP BY category
        """ % ','.join(['%s'] * len(product_ids))

        cursor.execute(positive_query, tuple(product_ids))
        positive_results = cursor.fetchall()

        # âœ… ê°ì„± ë¶„ì„ ë°ì´í„° ê°€ê³µ
        topic_sentiment = {
            "ë‚´êµ¬ì„± ë° í’ˆì§ˆ": {"positive": 0, "neutral": 0, "negative": 0, "count": 0},
            "ë””ìì¸": {"positive": 0, "neutral": 0, "negative": 0, "count": 0},
            "ì‚¬ì´ì¦ˆ": {"positive": 0, "neutral": 0, "negative": 0, "count": 0},
            "ê°€ì„±ë¹„": {"positive": 0, "neutral": 0, "negative": 0, "count": 0},
            "ì°©ìš©ê°": {"positive": 0, "neutral": 0, "negative": 0, "count": 0},
            "ë°°ì†¡ ë° í¬ì¥ ë° ì‘ëŒ€": {"positive": 0, "neutral": 0, "negative": 0, "count": 0}
        }

        # âœ… ë¶€ì •(1) ë°ì´í„° ë°˜ì˜
        for review in negative_results:
            category = review["category"]
            if category in topic_sentiment:
                topic_sentiment[category]["negative"] = review["count"]
                topic_sentiment[category]["count"] += review["count"]

        # âœ… ì¤‘ë¦½(0) ë°ì´í„° ë°˜ì˜
        for review in neutral_results:
            category = review["category"]
            if category in topic_sentiment:
                topic_sentiment[category]["neutral"] = review["count"]
                topic_sentiment[category]["count"] += review["count"]

        # âœ… ê¸ì •(2) ë°ì´í„° ë°˜ì˜
        for review in positive_results:
            category = review["category"]
            if category in topic_sentiment:
                topic_sentiment[category]["positive"] = review["count"]
                topic_sentiment[category]["count"] += review["count"]


        # âœ… ë¸Œëœë“œë³„ ë¦¬ë·° ê°œìˆ˜ (ì—°ë„ë³„ ì§‘ê³„)
        brand_query = """
        SELECT YEAR(r.review_date) AS review_year, 
               COUNT(*) AS total_reviews
        FROM reviews r
        JOIN products p ON r.product_id = p.product_id
        WHERE p.brand = %s
        GROUP BY review_year
        ORDER BY review_year
        """

        cursor.execute(brand_query, (brand,))
        sales_data = cursor.fetchall()

        if not sales_data:
            return {"message": "â— No sales data found", "sales": []}

        # âœ… ë°ì´í„° ë³€í™˜: { "2019": 150, "2020": 200, "2021": 300 }
        sales_dict = {str(row["review_year"]): row["total_reviews"] for row in sales_data} if sales_data else {}
        # âœ… ì ìˆ˜ ì €ì¥: brand_scores í…Œì´ë¸”ì— ì €ì¥

        return {
            "message": "âœ… Success",
            "overall_score": overall_score,
            "rating": get_brand_rating(overall_score),  # ë“±ê¸‰ 
            "sales_data": sales_dict,
            "brand": brand,
            "topics": topic_aggregated,
            "raw_data": results,
            "topic_sentiment": topic_sentiment
        }

    except pymysql.MySQLError as e:
        print(f"ğŸ”¥ MySQL Error: {str(e)}")
        return {"message": "ğŸ”¥ MySQL Error", "topics": {}, "raw_data": []}

    except Exception as e:
        print(f"ğŸ”¥ Unexpected Error: {str(e)}")
        return {"message": "ğŸ”¥ Unexpected Error", "topics": {}, "raw_data": []}


# ë¸Œëœë“œ ê²€ìƒ‰ API (ì‚¬ìš©ì ì…ë ¥í•œ ë¬¸ìì—´ í¬í•¨í•˜ëŠ” ë¸Œëœë“œ ëª©ë¡ ê²€ìƒ‰)
@app.get("/api/search-brands/")
async def search_brands(query: str):
    conn = get_mysql_connection()
    cursor = conn.cursor(pymysql.cursors.DictCursor)

    search_query = """
    SELECT DISTINCT brand 
    FROM products 
    WHERE brand LIKE %s 
    LIMIT 10
    """
    cursor.execute(search_query, (f"%{query}%",))
    brands = cursor.fetchall()

    return {"brands": [b["brand"] for b in brands]}


#--------------------------------------------------------#

#--------------------------------------------------------#
# ê°•ë¯¼ì„± - ê°•ì•½ì  ì‘ì—…


# ìƒí’ˆëª… ê²€ìƒ‰ ì¶”ì²œ API (ì˜ˆìƒ ê²€ìƒ‰ì–´ ì œì•ˆ)
@app.get("/api/suggest-products/{query}", response_model=List[Dict[str, Any]])
async def suggest_products(query: str):
    """
    ì‚¬ìš©ìê°€ ì…ë ¥í•œ query(ì œí’ˆëª… ì¼ë¶€)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ products í…Œì´ë¸”ì—ì„œ ìœ ì‚¬í•œ ìƒí’ˆëª…ì„ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œ ëª©ë¡ì„ ë°˜í™˜.
    """
    # MySQL ì—°ê²°
    connection = get_mysql_connection()
    if not connection:
        raise HTTPException(status_code=500, detail="Database connection failed")

    try:
        with connection.cursor() as cursor:
            # LIKEë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬í•œ ìƒí’ˆëª… ê²€ìƒ‰ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
            sql = """
                SELECT product_id, product_name 
                FROM products 
                WHERE product_name LIKE %s 
                LIMIT 5
            """
            # %query%ë¡œ ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰
            search_query = f"%{query}%"
            cursor.execute(sql, (search_query,))
            results = cursor.fetchall()

            if not results:
                return []  # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

            # ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜ (product_idì™€ product_nameë§Œ í¬í•¨)
            suggestions = [
                {
                    "product_id": row["product_id"],
                    "product_name": row["product_name"]
                }
                for row in results
            ]
            return suggestions

    except pymysql.MySQLError as e:
        print(f"MySQL Error: {e}")
        raise HTTPException(status_code=500, detail="Error fetching product suggestions")

    finally:
        connection.close()

# ìƒí’ˆëª…ìœ¼ë¡œ product_id ì¡°íšŒ API
@app.get("/api/get-product-id/{product_name}", response_model=Dict[str, Any])
async def get_product_id(product_name: str):
    """
    ìƒí’ˆëª…ì„ ì…ë ¥ë°›ì•„ í•´ë‹¹ ìƒí’ˆì˜ product_idë¥¼ ë°˜í™˜.
    """
    # MySQL ì—°ê²°
    connection = get_mysql_connection()
    if not connection:
        raise HTTPException(status_code=500, detail="Database connection failed")

    try:
        with connection.cursor() as cursor:
            # ìƒí’ˆëª…ìœ¼ë¡œ product_id ì¡°íšŒ (ì •í™•í•œ ì¼ì¹˜)
            sql = """
                SELECT product_id, product_name 
                FROM products 
                WHERE product_name = %s 
                LIMIT 1
            """
            cursor.execute(sql, (product_name,))
            result = cursor.fetchone()

            if not result:
                raise HTTPException(status_code=404, detail="Product not found")

            # ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜
            return {
                "product_id": result["product_id"],
                "product_name": result["product_name"]
            }

    except pymysql.MySQLError as e:
        print(f"MySQL Error: {e}")
        raise HTTPException(status_code=500, detail="Error fetching product ID")

    finally:
        connection.close()


# idë¡œ ë¸Œëœë“œ ì ìˆ˜ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œ ì¶”ê°€
@app.get("/api/brand-analysis-kang/{product_id}")
async def brand_analysis_kang(product_id: int) -> Dict[str, Any]:
    try:
        conn = get_mysql_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)

        print(f"ğŸ” ë¸Œëœë“œ ë¶„ì„ ìš”ì²­ (ìƒí’ˆ ID ê¸°ì¤€): {product_id}")

        # âœ… 1. ìƒí’ˆ IDë¡œ ë¸Œëœë“œ ì¡°íšŒ
        brand_query = """
        SELECT brand FROM products WHERE product_id = %s
        """
        cursor.execute(brand_query, (product_id,))
        brand_result = cursor.fetchone()

        if not brand_result or not brand_result["brand"]:
            return {"message": "â— No brand found for this product", "topics": {}, "raw_data": []}

        brand = brand_result["brand"]
        print(f"âœ… ì¡°íšŒëœ ë¸Œëœë“œ: {brand}")

        # âœ… 2. ê¸°ì¡´ brand_analysis ë¡œì§ ìˆ˜í–‰
        product_query = """
        SELECT product_id, product_name, category
        FROM products
        WHERE brand = %s
        """
        cursor.execute(product_query, (brand,))
        products = cursor.fetchall()

        if not products:
            return {"message": "â— No products found for this brand", "topics": {}, "raw_data": []}

        product_ids = [p["product_id"] for p in products]

        # âœ… ê°ì„± ë¶„ì„ ë°ì´í„° ì¡°íšŒ
        query = """
        SELECT category, sentiment 
        FROM review_analysis 
        WHERE product_id IN (%s)
        """ % ','.join(['%s'] * len(product_ids))
        cursor.execute(query, tuple(product_ids))
        results = cursor.fetchall()

        if not results:
            return {"message": "â— No sentiment data found for this brand", "topics": {}, "raw_data": []}

        # âœ… ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì ìˆ˜ ê³„ì‚°, í† í”½ ë¶„ì„ ë“±)
        topic_scores = {
            "ë‚´êµ¬ì„± ë° í’ˆì§ˆ": [],
            "ë””ìì¸": [],
            "ì‚¬ì´ì¦ˆ": [],
            "ê°€ì„±ë¹„": [],
            "ì°©ìš©ê°": [],
            "ë°°ì†¡ ë° í¬ì¥ ë° ì‘ëŒ€": []
        }
        topic_counts = {key: 0 for key in topic_scores}  

        for review in results:
            category = review["category"]
            sentiment = review["sentiment"]

            if category and sentiment is not None:
                categories = category.split(",")  
                for cat in categories:
                    cat = cat.strip()
                    if cat in topic_scores:
                        topic_scores[cat].append(sentiment)
                        topic_counts[cat] += 1  

        # âœ… í† í”½ë³„ ë§Œì¡±ë„ ê³„ì‚°
        topic_aggregated = {
            topic: {
                "score": round((sum(scores) / len(scores)) * 50, 2) if scores else 0,
                "count": topic_counts[topic]
            }
            for topic, scores in topic_scores.items()
        }

        # âœ… ë¸Œëœë“œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        review_count = len(results)
        avg_sentiment = sum(r["sentiment"] for r in results) / review_count * 50 if review_count > 0 else 0

        if review_count < 30:
            review_weight = 0.7  
        elif review_count < 100:
            review_weight = 0.85
        elif review_count < 300:
            review_weight = 0.95
        else:
            review_weight = 1.0  

        recent_reviews = sum(1 for r in results if r["sentiment"] >= 0.7)  
        recent_ratio = recent_reviews / max(review_count, 1)  

        if recent_ratio < 0.3:
            recent_weight = 0.85  
        elif recent_ratio < 0.5:
            recent_weight = 0.95
        else:
            recent_weight = 1.0  

        min_topic_score = min(topic["score"] for topic in topic_aggregated.values() if topic["count"] > 0)

        if min_topic_score < 60:
            topic_weight = 0.8
        elif min_topic_score < 75:
            topic_weight = 0.9
        else:
            topic_weight = 1.0  

        overall_score = avg_sentiment * review_weight * recent_weight * topic_weight
        overall_score = min(100, round(overall_score, 2))
        
        def get_brand_rating(score: float) -> str:
            if score < 60:
                return "ë¯¸í¡"
            elif score < 80:
                return "ë³´í†µ"
            else:
                return "ì¢‹ìŒ"
        # âœ… brand_scoresì— ì €ì¥
        try:
            save_conn = get_mysql_connection()
            save_cursor = save_conn.cursor()
            save_cursor.execute("""
                REPLACE INTO brand_scores (brand, overall_score, rating, review_count)
                VALUES (%s, %s, %s, %s)
            """, (
                brand,
                overall_score,
                get_brand_rating(overall_score),
                review_count
            ))
            save_conn.commit()
            save_cursor.close()
            save_conn.close()
        except Exception as e:
            print(f"âŒ ë¸Œëœë“œ ì ìˆ˜ ì €ì¥ ì‹¤íŒ¨: {brand}, ì˜¤ë¥˜: {e}")
        return {
            "message": "âœ… Success",
            "overall_score": overall_score,
            "rating": get_brand_rating(overall_score),  
            "brand": brand,
            "topics": topic_aggregated,
            "raw_data": results
        }

    except pymysql.MySQLError as e:
        print(f"ğŸ”¥ MySQL Error: {str(e)}")
        return {"message": "ğŸ”¥ MySQL Error", "topics": {}, "raw_data": []}

    except Exception as e:
        print(f"ğŸ”¥ Unexpected Error: {str(e)}")
        return {"message": "ğŸ”¥ Unexpected Error", "topics": {}, "raw_data": []}




# ê°•ì•½ì  ë¶„ì„ í˜ì´ì§€ ë Œë”ë§
@app.get("/strength-weakness.html")
async def strength_weakness_page(request: Request):
    return templates.TemplateResponse("strength-weakness.html", {"request": request})

@app.get("/api/strength-weakness/{product_id}") 
async def strength_weakness(product_id: str) -> Dict[str, Any]:
    try:
        conn = get_mysql_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)  

        # print(f"ğŸŸ¢ ê²€ìƒ‰ ìš”ì²­: {product_id}")  # âœ… ìš”ì²­ëœ ìƒí’ˆ ID í™•ì¸

        # âœ… ì œí’ˆ ì •ë³´ ì¡°íšŒ
        product_query = """
        SELECT p.product_name, p.price, p.gender, p.category, p.brand, p.image_url,
               COUNT(r.review_id) as review_count, 
               AVG(r.rating) as avg_rating,
               SUM(CASE WHEN r.review_date >= DATE_SUB(CURDATE(), INTERVAL 3 MONTH) THEN 1 ELSE 0 END) AS recent_reviews,
               SUM(CASE WHEN r.review_date BETWEEN DATE_SUB(CURDATE(), INTERVAL 6 MONTH) AND DATE_SUB(CURDATE(), INTERVAL 3 MONTH) THEN 1 ELSE 0 END) AS past_reviews
        FROM products p
        LEFT JOIN review_analysis r ON p.product_id = r.product_id
        WHERE p.product_id = %s
        GROUP BY p.product_id
        """
        cursor.execute(product_query, (product_id,))
        product_info = cursor.fetchone()

        # print(f"ğŸ”µ ì œí’ˆ ì •ë³´: {product_info}")  # âœ… ì œí’ˆ ì •ë³´ í™•ì¸

        if not product_info:
            return {"message": "â— No product data found", "topics": {}, "product": {}, "raw_data": []}

        # âœ… ê°ì„± ë¶„ì„ ë°ì´í„° ì¡°íšŒ
        query = """
        SELECT category, sentiment 
        FROM review_analysis 
        WHERE product_id = %s
        """
        cursor.execute(query, (product_id,))
        results = cursor.fetchall()

        # print(f"ğŸŸ¡ ê°ì„± ë¶„ì„ ë°ì´í„°: {results}")  # âœ… ê°ì„± ë¶„ì„ ë°ì´í„° í™•ì¸

        if not results:
            return {"message": "â— No sentiment data found", "topics": {}, "product": product_info, "raw_data": []}

        # âœ… ê°ì„± ë¶„ì„ ë°ì´í„° ê°€ê³µ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
        topic_scores = {
            "ë‚´êµ¬ì„± ë° í’ˆì§ˆ": [],
            "ë””ìì¸": [],
            "ì‚¬ì´ì¦ˆ": [],
            "ê°€ì„±ë¹„": [],
            "ì°©ìš©ê°": [],
            "ë°°ì†¡ ë° í¬ì¥ ë° ì‘ëŒ€": []
        }
        topic_counts = {key: 0 for key in topic_scores}  

        for review in results:
            category = review["category"]
            sentiment = review["sentiment"]

            if category and sentiment is not None:
                categories = category.split(",")  
                for cat in categories:
                    cat = cat.strip()
                    if cat in topic_scores:
                        topic_scores[cat].append(sentiment)
                        topic_counts[cat] += 1  

        topic_aggregated = {
            topic: {
                "score": round((sum(scores) / len(scores)) * 50, 2) if scores else 0,
                "count": topic_counts[topic]
            }
            for topic, scores in topic_scores.items()
        }

        # print(f"ğŸŸ  ë¶„ì„ ê²°ê³¼: {topic_aggregated}")  # âœ… ê°ì„± ë¶„ì„ ì ìˆ˜ í™•ì¸

        # âœ… ì¶”ê°€ëœ ì¢…í•© ì ìˆ˜ ê³„ì‚° (ìƒˆë¡œìš´ ê¸°ì¤€ ë°˜ì˜)
        avg_sentiment = sum(r["sentiment"] for r in results) / len(results) * 50  # ê°ì„± ì ìˆ˜ (0~100 ë³€í™˜)
        review_count = product_info["review_count"]
        avg_rating = product_info["avg_rating"]
        recent_reviews = product_info["recent_reviews"]
        past_reviews = product_info["past_reviews"]

        overall_score = avg_sentiment  # ê¸°ë³¸ ì ìˆ˜ (ê°ì„± ì ìˆ˜)

        # âœ… ë¦¬ë·° ê°œìˆ˜ ê¸°ì¤€ ì ìš©
        if review_count < 50:
            overall_score *= 0.8
        elif review_count < 150:
            overall_score *= 0.9

        # âœ… ìµœê·¼ ë¦¬ë·° ë¹„ìœ¨ ì ìš©
        if past_reviews > 0 and recent_reviews / past_reviews >= 1:
            overall_score *= 0.9

        # âœ… íŠ¹ì • í† í”½ ì ìˆ˜ ì ìš©
        min_topic_score = min(topic["score"] for topic in topic_aggregated.values() if topic["count"] > 0)
        if min_topic_score <= 80:
            overall_score *= 0.9

        # âœ… í‰ì  ì ìš©
        if avg_rating and avg_rating < 4.7:
            overall_score *= 0.95

        # âœ… ìµœëŒ€ 100ì  ì œí•œ
        overall_score = min(100, round(overall_score, 2))

        # print(f"ğŸŸ¢ ìµœì¢… ì¢…í•© ì ìˆ˜: {overall_score}")  # âœ… í™•ì¸ìš© ë¡œê·¸

        # âœ… ì ìˆ˜ ì €ì¥: product_scores í…Œì´ë¸”ì— ì €ì¥
        try:
            save_conn = get_mysql_connection()
            save_cursor = save_conn.cursor()
            save_cursor.execute("""
                REPLACE INTO product_scores (product_id, product_name, brand, overall_score)
                VALUES (%s, %s, %s, %s)
            """, (
                product_id,
                product_info["product_name"],
                product_info["brand"],
                overall_score
            ))
            save_conn.commit()
            save_cursor.close()
            save_conn.close()
        except Exception as e:
            print(f"âŒ ì œí’ˆ ì ìˆ˜ ì €ì¥ ì‹¤íŒ¨: {product_id}, ì˜¤ë¥˜: {e}")

        return {
            "message": "âœ… Success",
            "overall_score": overall_score,  # âœ… ì¶”ê°€ëœ ë¶€ë¶„
            "topics": topic_aggregated,
            "product": {
                "product_name": product_info["product_name"],
                "price": product_info["price"],
                "gender": product_info["gender"],
                "category": product_info["category"],
                "brand": product_info["brand"],
                "image_url": product_info["image_url"],
                "review_count": product_info["review_count"],
                "avg_rating": round(product_info["avg_rating"], 2) if product_info["avg_rating"] else 0
            },
            "raw_data": results
        }

    except pymysql.MySQLError as e:
        print(f"ğŸ”¥ MySQL Error: {str(e)}")
        return {"message": "ğŸ”¥ MySQL Error", "topics": {}, "product": {}, "raw_data": []}

    except Exception as e:
        print(f"ğŸ”¥ Unexpected Error: {str(e)}")
        return {"message": "ğŸ”¥ Unexpected Error", "topics": {}, "product": {}, "raw_data": []}

MIN_REVIEW_COUNT = 30  # âœ… ìµœì†Œ ë¦¬ë·° ê°œìˆ˜ ì œí•œ

# âœ… MySQL ì—°ê²° í’€ë§ (ì´ˆê¸°í™”)
async def create_mysql_pool():
    return await aiomysql.create_pool(
        host="15.152.242.221",
        port=3306,
        user="admin",
        password="Admin@1234",
        db="musinsa_pd_rv",
        cursorclass=aiomysql.DictCursor,
        minsize=1,
        maxsize=10
    )

@app.post("/api/update-all-brand-scores")
async def update_all_brand_scores():
    if not mysql_pool:
        return {"message": "â— DB ì—°ê²° ì‹¤íŒ¨"}

    async with mysql_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            await cursor.execute("SELECT DISTINCT brand FROM products WHERE brand IS NOT NULL")
            brands = await cursor.fetchall()

    if not brands:
        return {"message": "â— ë¸Œëœë“œ ì—†ìŒ"}

    for b in brands:
        brand = b["brand"]

        async with mysql_pool.acquire() as conn:
            async with conn.cursor(aiomysql.DictCursor) as cursor:
                await cursor.execute("SELECT product_id FROM products WHERE brand = %s", (brand,))
                product_rows = await cursor.fetchall()
                if not product_rows:
                    continue

                product_ids = [p["product_id"] for p in product_rows]
                if not product_ids:
                    continue

                format_strings = ','.join(['%s'] * len(product_ids))
                await cursor.execute(f"""
                    SELECT category, sentiment
                    FROM review_analysis
                    WHERE product_id IN ({format_strings})
                """, tuple(product_ids))
                reviews = await cursor.fetchall()

        if not reviews:
            overall_score = 0.0
            rating = "ë¯¸í¡"
            review_count = 0
        else:
            # ì ìˆ˜ ê³„ì‚° ë¡œì§ (brand-analysis APIì™€ ë™ì¼)
            topic_scores = {
                "ë‚´êµ¬ì„± ë° í’ˆì§ˆ": [], "ë””ìì¸": [], "ì‚¬ì´ì¦ˆ": [],
                "ê°€ì„±ë¹„": [], "ì°©ìš©ê°": [], "ë°°ì†¡ ë° í¬ì¥ ë° ì‘ëŒ€": []
            }

            for review in reviews:
                if review["category"] and review["sentiment"] is not None:
                    for cat in review["category"].split(","):
                        cat = cat.strip()
                        if cat in topic_scores:
                            topic_scores[cat].append(review["sentiment"])

            topic_aggregated = {
                topic: {
                    "score": round((sum(scores) / len(scores)) * 50, 2) if scores else 0
                }
                for topic, scores in topic_scores.items()
            }

            review_count = len(reviews)
            avg_sentiment = sum(r["sentiment"] for r in reviews) / review_count * 50

            review_weight = 0.7 if review_count < 30 else 0.85 if review_count < 100 else 0.95 if review_count < 300 else 1.0
            recent_positive = sum(1 for r in reviews if r["sentiment"] >= 0.7)
            recent_ratio = recent_positive / review_count
            recent_weight = 0.85 if recent_ratio < 0.3 else 0.95 if recent_ratio < 0.5 else 1.0
            min_topic_score = min((t["score"] for t in topic_aggregated.values() if t["score"] > 0), default=0)
            topic_weight = 0.8 if min_topic_score < 60 else 0.9 if min_topic_score < 75 else 1.0

            overall_score = avg_sentiment * review_weight * recent_weight * topic_weight
            overall_score = min(100, round(overall_score, 2))
            rating = "ë¯¸í¡" if overall_score < 60 else "ë³´í†µ" if overall_score < 80 else "ì¢‹ìŒ"

        async with mysql_pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute("""
                    REPLACE INTO brand_scores (brand, overall_score, rating, review_count)
                    VALUES (%s, %s, %s, %s)
                """, (
                    brand,
                    overall_score,
                    rating,
                    review_count
                ))
                await conn.commit()

    return {"message": "âœ… ëª¨ë“  ë¸Œëœë“œ ì ìˆ˜ ê³„ì‚° ë° ì €ì¥ ì™„ë£Œ"}

mysql_pool = None

@app.on_event("startup")
async def startup_event():
    global mysql_pool
    mysql_pool = await create_mysql_pool()

    # ì„œë²„ ì‹œì‘ë  ë•Œ ìë™ ì‹¤í–‰
    await update_all_brand_scores()

@app.on_event("shutdown")
async def shutdown_event():
    global mysql_pool
    if mysql_pool:
        mysql_pool.close()
        await mysql_pool.wait_closed()

async def fetch_rankings():
    async with mysql_pool.acquire() as conn:
        async with conn.cursor() as cursor:
            await cursor.execute("""
                SELECT brand, overall_score, rating, review_count
                FROM brand_scores
                WHERE review_count >= %s
                ORDER BY overall_score DESC
            """, (MIN_REVIEW_COUNT,))
            return await cursor.fetchall()

async def fetch_product_rankings():
    async with mysql_pool.acquire() as conn:
        async with conn.cursor() as cursor:
            await cursor.execute("""
                SELECT product_id, product_name, brand, overall_score
                FROM product_scores
                WHERE overall_score > 0
                ORDER BY overall_score DESC
            """)
            return await cursor.fetchall()

from decimal import Decimal

def convert_decimal(obj):
    if isinstance(obj, Decimal):
        return float(obj)
    if isinstance(obj, list):
        return [convert_decimal(i) for i in obj]
    if isinstance(obj, dict):
        return {k: convert_decimal(v) for k, v in obj.items()}
    return obj

@app.get("/api/rankings")
async def get_rankings():
    brands = await fetch_rankings()
    products = await fetch_product_rankings()

    result = {
        "message": "âœ… Success",
        "top_brands": convert_decimal(brands[:3]),
        "bottom_brands": convert_decimal(brands[-3:]),
        "top_products": convert_decimal(products[:3]),
        "bottom_products": convert_decimal(products[-3:])
    }

    return JSONResponse(content=result, media_type="application/json")


    
# âœ… ì œí’ˆ ê²€ìƒ‰ API (íŠ¹ì • í† í”½ì´ ìµœê³ ì ì´ê±°ë‚˜ ê°ì„± ì ìˆ˜ê°€ 93ì  ì´ìƒì¼ ë•Œë§Œ í•„í„°ë§)
@app.get("/api/product-search")
async def product_search(
    category: str = "",
    gender: str = "",
    price_min: int = 0,
    price_max: int = 99999999,
    topic: str = "",
    min_reviews: int = 0
):
    try:
        conn = get_mysql_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)

        # âœ… ì„±ë³„ í•„í„°ë§ ë¡œì§ ìˆ˜ì •
        gender_filter = {
            "ë‚¨ì": "('ë‚¨ì', 'ê³µìš©')",
            "ì—¬ì": "('ì—¬ì', 'ê³µìš©')",
            "ê³µìš©": "('ë‚¨ì', 'ì—¬ì', 'ê³µìš©')"
        }
        gender_condition = f"p.gender IN {gender_filter.get(gender, gender_filter['ê³µìš©'])}"  # ê¸°ë³¸ê°’: ì „ì²´ í¬í•¨

        # âœ… 1ì°¨ í•„í„°ë§: ì œí’ˆ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        query = f"""
        SELECT p.product_id, p.product_name, p.price, p.category, p.brand, p.gender, p.image_url,
               COUNT(r.review_id) as review_count,
               AVG(r.sentiment) * 50 as avg_sentiment  -- âœ… ì „ì²´ ê°ì„± ì ìˆ˜ í‰ê·  ì¶”ê°€
        FROM products p
        JOIN review_analysis r ON p.product_id = r.product_id
        WHERE (p.category = %s OR %s = '')  
          AND {gender_condition}  -- âœ… ì„±ë³„ í•„í„°ë§ ìˆ˜ì •
          AND CAST(REPLACE(p.price, ',', '') AS UNSIGNED) BETWEEN %s AND %s
        GROUP BY p.product_id
        HAVING review_count >= %s
        """
        cursor.execute(query, (category, category, price_min, price_max, min_reviews))
        product_results = cursor.fetchall()

        if not product_results:
            return {"products": []}

        product_ids = [row["product_id"] for row in product_results]

        # âœ… 2ì°¨ í•„í„°ë§: ê°ì„± ì ìˆ˜ ê³„ì‚° (í† í”½ë³„ ë¶„ë¦¬ í›„ ê°ì„± ë¶„ì„)
        sentiment_query = """
        SELECT product_id, category, sentiment
        FROM review_analysis
        WHERE product_id IN (%s)
        """ % ",".join(["%s"] * len(product_ids))

        cursor.execute(sentiment_query, product_ids)
        sentiment_results = cursor.fetchall()

        # âœ… ì œí’ˆë³„ í† í”½ ê°ì„± ì ìˆ˜ ì €ì¥
        product_data = {row["product_id"]: {**row, "topics": {}} for row in product_results}

        for row in sentiment_results:
            product_id = row["product_id"]
            categories = row["category"].split(",")  # âœ… ì—¬ëŸ¬ ê°œì˜ í† í”½ì„ ë¶„ë¦¬
            sentiment = row["sentiment"]

            for cat in categories:
                cat = cat.strip()
                if cat not in product_data[product_id]["topics"]:
                    product_data[product_id]["topics"][cat] = []
                product_data[product_id]["topics"][cat].append(sentiment)

        # âœ… íŠ¹ì • í† í”½ í•„í„°ë§ ì ìš© ë° ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        filtered_products = []
        for product in product_data.values():
            topic_scores = {
                topic: round((sum(scores) / len(scores)) * 50, 2)
                for topic, scores in product["topics"].items()
            }

            product["filtered_topics"] = {}

            # âœ… íŠ¹ì • í† í”½ í•„í„°ë§ì´ ì ìš©ëœ ê²½ìš°, ìµœê³  ì ìˆ˜ or 93ì  ì´ìƒì¸ ê²½ìš°ë§Œ í¬í•¨
            if topic:
                if topic in topic_scores and (topic_scores[topic] >= 93 or topic_scores[topic] == max(topic_scores.values())):
                    product["filtered_topics"][topic] = topic_scores[topic]
                else:
                    continue  # âœ… ì¡°ê±´ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ ì•ˆí•¨
            else:
                product["filtered_topics"] = topic_scores  # âœ… í•„í„°ë§ì´ ì—†ì„ ë•ŒëŠ” ì „ì²´ í¬í•¨

            product["avg_sentiment"] = round(product["avg_sentiment"], 2) if product["avg_sentiment"] else 0

            filtered_products.append(product)

        return {"products": filtered_products}

    except pymysql.MySQLError as e:
        return {"error": f"ğŸ”¥ MySQL Error: {str(e)}"}
    except Exception as e:
        return {"error": f"ğŸ”¥ Unexpected Error: {str(e)}"}



@app.get("/api/related-reviews/{product_id}")
async def get_related_reviews(product_id: str):
    conn = None
    cursor = None
    try:
        # âœ… MySQL ì—°ê²° í™•ì¸
        conn = get_mysql_connection()
        if not conn:
            print("ğŸ”¥ MySQL ì—°ê²° ì‹¤íŒ¨")
            return {"error": "ğŸ”¥ MySQL ì—°ê²° ì‹¤íŒ¨"}, 500

        cursor = conn.cursor()

        print(f"ğŸ” ë¦¬ë·° ê²€ìƒ‰ ìš”ì²­: {product_id}")

        # âœ… 1. ìƒí’ˆ ë¦¬ë·° ê°œìˆ˜ í™•ì¸
        cursor.execute("""
            SELECT COUNT(*) AS review_count 
            FROM review_analysis 
            WHERE product_id = %s
        """, (product_id,))
        review_count = cursor.fetchone()["review_count"]

        if review_count == 0:
            print(f"ğŸš¨ í•´ë‹¹ ìƒí’ˆ({product_id}) ë¦¬ë·° ì—†ìŒ")
            return {"reviews": []}

        # âœ… 2. í† í”½ë³„ ë§Œì¡±ë„ ì ìˆ˜ ë° ë¹ˆë„ìˆ˜ ì¡°íšŒ
        cursor.execute("""
            SELECT category, 
                   ROUND(AVG(sentiment) * 50, 2) AS score, 
                   COUNT(*) AS freq 
            FROM review_analysis
            WHERE product_id = %s
            GROUP BY category
        """, (product_id,))
        topic_data = cursor.fetchall()

        if not topic_data:
            return {"reviews": []}

        # âœ… 3. ë§Œì¡±ë„ í•˜ìœ„ 3ê°œ + ì–¸ê¸‰ëŸ‰ ìƒìœ„ 3ê°œ í† í”½ í¬í•¨ (ëª¨ë“  ì¡°ê±´ ì¶©ì¡±)
        sorted_by_score = sorted(topic_data, key=lambda x: x["score"])[:3]
        sorted_by_freq = sorted(topic_data, key=lambda x: x["freq"], reverse=True)[:3]

        lowest_score_topics = {t["category"] for t in sorted_by_score}
        highest_freq_topics = {t["category"] for t in sorted_by_freq}

        # âœ… 4. ëª¨ë“  í† í”½ í¬í•¨ (êµì§‘í•© + ê°œë³„ ì¡°ê±´ ì¶©ì¡±í•œ ê²ƒë“¤)
        selected_topics = lowest_score_topics | highest_freq_topics  # í•©ì§‘í•©

        if not selected_topics:
            print("ğŸš¨ ì ì ˆí•œ í† í”½ ì—†ìŒ")
            return {"reviews": []}

        print(f"ğŸŸ¢ ì„ íƒëœ í† í”½ë“¤: {selected_topics}")

        # âœ… 5. ì„ íƒëœ ëª¨ë“  í† í”½ì„ í¬í•¨í•˜ëŠ” ë¶€ì •/ì¤‘ë¦½ ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
        topic_conditions = " OR ".join([f"ra.category LIKE '%%{topic}%%'" for topic in selected_topics])

        query = f"""
            SELECT r.review_id, r.review_text, r.rating, r.review_date, ra.sentiment
            FROM reviews r
            JOIN review_analysis ra ON r.review_id = ra.review_id
            WHERE ra.product_id = %s 
              AND ({topic_conditions})
              AND (ra.sentiment = 0 OR ra.sentiment = 1)  # ë¶€ì •(0) > ì¤‘ë¦½(1) ìš°ì„ 
            ORDER BY ra.sentiment ASC, CHAR_LENGTH(r.review_text) DESC
            LIMIT 50
        """
        cursor.execute(query, (product_id,))
        reviews = cursor.fetchall()

        if not reviews:
            print("ğŸš¨ ì„ íƒëœ í† í”½ì˜ ë¶€ì •/ì¤‘ë¦½ ë¦¬ë·° ì—†ìŒ")
            return {"reviews": []}

        print(f"ğŸŸ¢ ê°€ì ¸ì˜¨ ë¦¬ë·° ê°œìˆ˜: {len(reviews)}")

        return {"reviews": reviews}

    except pymysql.MySQLError as e:
        print(f"ğŸ”¥ MySQL Error: {str(e)}")
        return {"error": "ğŸ”¥ MySQL Error"}, 500

    except Exception as e:
        print(f"ğŸ”¥ Unexpected Error: {str(e)}")
        return {"error": "ğŸ”¥ Unexpected Error"}, 500

    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()


from fastapi import APIRouter

router = APIRouter()

from dateutil.relativedelta import relativedelta

@app.get("/api/review-trend/{product_id}")
async def get_review_trend(product_id: str):
    """ìƒí’ˆ ID ê¸°ì¤€ìœ¼ë¡œ ìµœê·¼ 1ë…„ ë°˜ ë™ì•ˆì˜ ì›”ë³„ ë¦¬ë·° ê°œìˆ˜ë¥¼ ë°˜í™˜"""

    conn = None
    cursor = None

    try:
        # âœ… MySQL ì—°ê²°
        conn = get_mysql_connection()
        if not conn:
            return {"error": "ğŸ”¥ MySQL ì—°ê²° ì‹¤íŒ¨"}, 500

        cursor = conn.cursor(pymysql.cursors.DictCursor)

        # âœ… SQL ì‹¤í–‰ (ì›”ë³„ ë¦¬ë·° ê°œìˆ˜ ì§‘ê³„)
        query = """
        SELECT DATE_FORMAT(review_date, '%%Y-%%m') AS review_month, COUNT(*) AS review_count
        FROM reviews
        WHERE product_id = %s 
          AND review_date >= DATE_SUB(CURDATE(), INTERVAL 18 MONTH)
        GROUP BY review_month
        ORDER BY review_month;
        """
        cursor.execute(query, (product_id,))
        results = cursor.fetchall()

        # âœ… ê²°ê³¼ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
        if not results:
            return {"product_id": product_id, "review_trend": {}}

        # âœ… ë”•ì…”ë„ˆë¦¬ ë³€í™˜ (ë¹ˆ ì›” í¬í•¨)
        from dateutil.relativedelta import relativedelta
        from datetime import datetime

        end_date = datetime.today().replace(day=1)  # ì´ë²ˆ ë‹¬ 1ì¼
        start_date = end_date - relativedelta(months=18)  # 18ê°œì›” ì „

        trend_data = {}
        current_date = start_date

        while current_date <= end_date:
            key = current_date.strftime("%Y-%m")
            trend_data[key] = 0  # ê¸°ë³¸ê°’ 0
            current_date += relativedelta(months=1)  # 1ê°œì›” ì¦ê°€

        # âœ… ê²°ê³¼ê°’ ë°˜ì˜
        for row in results:
            trend_data[row["review_month"]] = row["review_count"]

        return {"product_id": product_id, "review_trend": trend_data}

    except pymysql.MySQLError as e:
        return {"error": f"ğŸ”¥ MySQL Error: {str(e)}"}, 500

    except Exception as e:
        return {"error": f"ğŸ”¥ Unexpected Error: {str(e)}"}, 500

    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()




# ì¶”ê°€
@app.get("/api/size-analysis-product/{product_id}")
async def get_size_analysis(product_id: str) -> Dict[str, Any]:
    try:
        conn = get_mysql_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)

        query = """
        SELECT review_size, total_reviews, positive_count, neutral_count, negative_count
        FROM size_analysis_view
        WHERE product_id = %s
        """
        cursor.execute(query, (product_id,))
        results = cursor.fetchall()
        cursor.close()
        conn.close()

        size_data = []
        for row in results:
            total_reviews = row["total_reviews"]
            if total_reviews == 0:
                continue

            satisfaction_score = ((row["positive_count"] * 100) + (row["neutral_count"] * 50)) / total_reviews
            size_data.append({
                "review_size": row["review_size"],
                "satisfaction_score": round(satisfaction_score, 1),
                "total_reviews": total_reviews
            })

        # ìƒìœ„ 3ê°œ & í•˜ìœ„ 3ê°œ ì •ë ¬
        size_data_sorted = sorted(size_data, key=lambda x: x["satisfaction_score"], reverse=True)
        top_3 = size_data_sorted[:3]
        bottom_3 = size_data_sorted[-3:]

        return {
            "top_sizes": top_3,
            "bottom_sizes": bottom_3
        }

    except pymysql.MySQLError as e:
        return {"message": f"MySQL Error: {str(e)}"}

    except Exception as e:
        return {"message": f"Unexpected Error: {str(e)}"}

# ë² ì´ìŠ¤ í˜ì´ì§€ ë§Œë“¤ê¸°
# ğŸš€ `/dashboard/`ì—ì„œ base.html ë¶ˆëŸ¬ì˜¤ê¸°
@app.get("/dashboard/")
async def dashboard_page(request: Request):
    return templates.TemplateResponse("base.html", {"request": request})

# ğŸš€ AJAX ìš”ì²­ìœ¼ë¡œ index.html ë¶ˆëŸ¬ì˜¤ê¸°
@app.get("/dashboard/index/")
async def load_index_page(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

# ğŸš€ AJAX ìš”ì²­ìœ¼ë¡œ strength-weakness.html ë¶ˆëŸ¬ì˜¤ê¸°
@app.get("/dashboard/strength-weakness/")
async def load_strength_weakness_page(request: Request):
    return templates.TemplateResponse("strength-weakness.html", {"request": request})

# ê²€ìƒ‰ì „ì— ì¡´ì¬ ì—¬ë¶€ í™•ì¸
@app.get("/api/check-product/{product_id}")
async def check_product(product_id: str):
    """ âœ… ì œí’ˆ ID ì¡´ì¬ ì—¬ë¶€ í™•ì¸ API """
    conn = get_mysql_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database Connection Failed")
    
    try:
        with conn.cursor() as cursor:
            query = "SELECT COUNT(*) AS count FROM products WHERE product_id = %s"
            cursor.execute(query, (product_id,))
            result = cursor.fetchone()
        
        return {"exists": result["count"] > 0}  # âœ… ì¡´ì¬ ì—¬ë¶€ ë°˜í™˜
    
    except pymysql.MySQLError as e:
        print(f"ğŸ”¥ [DB Error] {e}")
        raise HTTPException(status_code=500, detail="Database Query Failed")
    
    finally:
        conn.close()



#--------------------------------------------------------#




#--------------------------------------------------------#
#ì´ë„í›ˆ - ë‚ ì§œ ë¶„ì„

from pydantic import BaseModel

class DateFilter(BaseModel):
    startDate: Optional[str] = None
    endDate: Optional[str] = None
    product_id: Optional[int] = None
    brand: Optional[str] = None

# ë‚ ì§œ ë¶„ì„ í˜ì´ì§€ ë Œë”ë§
@app.get("/date_analysis.html")
async def dashboard(request: Request):
    print(f"ğŸ”¹ ìš”ì²­ ë°›ìŒ: {request}")  # ìš”ì²­ ê°ì²´ í™•ì¸
    return templates.TemplateResponse("date_analysis.html", {"request": request})

# ë¸Œëœë“œ ëª©ë¡ API
@app.get("/api/brands")
async def get_brands():
    conn = None
    cursor = None
    
    try:
        conn = get_mysql_connection()
        cursor = conn.cursor()

        # ë¸Œëœë“œ ëª©ë¡ ì¡°íšŒ
        query = "SELECT DISTINCT brand FROM products ORDER BY brand"
        cursor.execute(query)
        result = cursor.fetchall()

        # ë””ë²„ê¹…ìš© ë¡œê·¸ ì¶œë ¥
        print("ğŸ” ë¸Œëœë“œ ì¡°íšŒ ê²°ê³¼:", result)

        # ë¸Œëœë“œ ëª©ë¡ ë°˜í™˜
        brands = [row["brand"] for row in result]
        return {"brands": brands}

    except pymysql.Error as e:  # âœ… mysql.connector ì˜ˆì™¸ ì²˜ë¦¬
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"Database error: {e}")
            
    except Exception as e:
        print(f"âŒ ë¸Œëœë“œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {"error": "Failed to fetch brands"}, 500

    finally:  
        if cursor:
            cursor.close()
        if conn:
            conn.close()
# =====================================================================================================                
# ë‚ ì§œ ë¶„ì„ API
@app.post("/api/date_analysis")
async def analyze_reviews(filter: DateFilter):
    print(f"âœ… ë‚ ì§œ ë¶„ì„ API í˜¸ì¶œë¨: {filter}")


    conn = get_mysql_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Failed to connect to the database")

    cursor = conn.cursor()

    # ì¡°ê±´ì— ë”°ë¼ SQL ì¿¼ë¦¬ ë¶„ê¸° ì²˜ë¦¬
    if filter.brand and filter.startDate and filter.endDate:
        # Case 1: ë¸Œëœë“œ + ë‚ ì§œ
        query = """
            SELECT DATE(r.review_date) AS date, COUNT(*) AS count 
            FROM review_analysis r
            JOIN products p ON r.product_id = p.product_id 
            WHERE p.brand = %s 
              AND review_date BETWEEN %s AND %s
              AND review_date BETWEEN '2017-06-01' AND '2025-01-31'
            GROUP BY DATE(r.review_date)
            ORDER BY date
        """
        params = [filter.brand, filter.startDate, filter.endDate]
    elif filter.brand:
        # Case 2: ë¸Œëœë“œ ë‹¨ë… (ì „ì²´ ê¸°ê°„)
        query = """
            SELECT DATE(r.review_date) AS date, COUNT(*) AS count 
            FROM review_analysis r
            JOIN products p ON r.product_id = p.product_id
            WHERE p.brand = %s
              AND review_date BETWEEN '2017-06-01' AND '2025-01-31'
            GROUP BY DATE(r.review_date)
            ORDER BY date
        """
        params = [filter.brand]


    elif filter.product_id and filter.startDate and filter.endDate:
        # 1. product_id, startDate, endDate ëª¨ë‘ ì…ë ¥ëœ ê²½ìš°
        query = """
            SELECT DATE(review_date) AS date, COUNT(*) AS count 
            FROM review_analysis 
            WHERE review_date BETWEEN %s AND %s
              AND product_id = %s
              AND review_date BETWEEN '2017-06-01' AND '2025-01-31'              
            GROUP BY DATE(review_date)
            ORDER BY date
        """
        params = [filter.startDate, filter.endDate, filter.product_id]
    elif filter.product_id:
        # 2. product_idë§Œ ì…ë ¥ëœ ê²½ìš° (ì „ì²´ ë‚ ì§œ)
        query = """
            SELECT DATE(review_date) AS date, COUNT(*) AS count 
            FROM review_analysis 
            WHERE product_id = %s
              AND review_date BETWEEN '2017-06-01' AND '2025-01-31'            
            GROUP BY DATE(review_date)
            ORDER BY date
        """
        params = [filter.product_id]
    elif filter.startDate and filter.endDate:
        # 3. startDateì™€ endDateë§Œ ì…ë ¥ëœ ê²½ìš° (ëª¨ë“  ì œí’ˆ)
        query = """
            SELECT DATE(review_date) AS date, COUNT(*) AS count 
            FROM review_analysis 
            WHERE review_date BETWEEN %s AND %s
               AND review_date BETWEEN '2017-06-01' AND '2025-01-31'           
            GROUP BY DATE(review_date)
            ORDER BY date
        """
        params = [filter.startDate, filter.endDate]
    else:
        # 4. ì•„ë¬´ ì¡°ê±´ë„ ì…ë ¥ë˜ì§€ ì•Šì€ ê²½ìš° ì—ëŸ¬ ë°˜í™˜
        raise HTTPException(status_code=400, detail="ë¸Œëœë“œ/ì œí’ˆID ë˜ëŠ” ë‚ ì§œ í•„ìˆ˜") 

    try:
        # ì¿¼ë¦¬ ì¶œë ¥
        print("ğŸ” ì‹¤í–‰í•  SQL ì¿¼ë¦¬:", query)
        print("ğŸ“Œ ë°”ì¸ë”©í•  íŒŒë¼ë¯¸í„°:", params)

        cursor.execute(query, params)
        result = cursor.fetchall()

        # ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°
        if not result:
            return {"labels": [], "values": []}

        # ê²°ê³¼ ë°˜í™˜
        return {
            "labels": [row["date"].strftime("%Y-%m-%d") for row in result],
            "values": [row["count"] for row in result]
        }

    except pymysql.ProgrammingError as e:
        print(f"âŒ MySQL Programming Error: {e}")
        raise HTTPException(status_code=500, detail=f"MySQL Programming Error: {str(e)}")

    except pymysql.OperationalError as e:
        print(f"âŒ MySQL Connection Error: {e}")
        raise HTTPException(status_code=500, detail=f"MySQL Connection Error: {str(e)}")

    except Exception as e:
        print(f"âŒ Unexpected Error: {e}")
        raise HTTPException(status_code=500, detail=f"Unexpected Error: {str(e)}")

    finally:
        if cursor:  # âœ… ì•ˆì „í•œ í™•ì¸ ë°©ì‹
            cursor.close()
        if conn:
            conn.close()

from typing import Optional
from fastapi import Query, HTTPException
import pymysql

# ğŸ”¹ MySQL ì—°ê²° í•¨ìˆ˜
def get_mysql_connection():
    try:
        return pymysql.connect(
            host="15.152.242.221",
            user="admin",
            password="Admin@1234",
            database="musinsa_pd_rv",
            cursorclass=pymysql.cursors.DictCursor
        )
    except pymysql.MySQLError as e:
        print(f"âŒ MySQL Connection Error: {e}")
        return None

# âœ… ì „ì²´ íŒë§¤ëŸ‰ API (ë¦¬ë·° ê°œìˆ˜ ê¸°ë°˜)
@app.get("/api/total-sales")
async def total_sales(startDate: Optional[str] = None, endDate: Optional[str] = None):
    """ì „ì²´ íŒë§¤ëŸ‰ ì¶”ì´ ë°ì´í„° (ë¦¬ë·° ê°œìˆ˜ ê¸°ë°˜)"""
    conn = get_mysql_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")

    cursor = conn.cursor()

    # ë¦¬ë·° ê°œìˆ˜ ê¸°ë°˜ íŒë§¤ëŸ‰ ì¡°íšŒ (ì „ì²´ ê¸°ê°„)
    query = """
        SELECT DATE(r.review_date) AS date, COUNT(*) AS sales
        FROM reviews r
        WHERE r.review_date BETWEEN '2017-06-01' AND '2025-01-31'
    """
    params = []

    # ë‚ ì§œ í•„í„° ì¶”ê°€
    if startDate and endDate:
        query += " AND r.review_date BETWEEN %s AND %s"
        params.extend([startDate, endDate])

    query += " GROUP BY DATE(r.review_date) ORDER BY date"

    cursor.execute(query, params)
    results = cursor.fetchall()

    cursor.close()
    conn.close()

    # ğŸ”¹ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
    print(f"ğŸ“Š ì´ íŒë§¤ëŸ‰ ë°ì´í„°: {results}")

    # ê²°ê³¼ ë°˜í™˜
    return {
        "labels": [row["date"].strftime("%Y-%m-%d") if row["date"] else "N/A" for row in results],
        "values": [row["sales"] for row in results]
    }

# âœ… ì¹´í…Œê³ ë¦¬ë³„ íŒë§¤ëŸ‰ API (ë¦¬ë·° ê°œìˆ˜ ê¸°ë°˜)
@app.get("/api/category-sales")
async def category_sales(startDate: Optional[str] = None, endDate: Optional[str] = None):
    """ì¹´í…Œê³ ë¦¬ë³„ íŒë§¤ëŸ‰ ì¶”ì´ ë°ì´í„° (ë¦¬ë·° ê°œìˆ˜ ê¸°ë°˜)"""
    conn = get_mysql_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")

    cursor = conn.cursor()

    # ë¦¬ë·° ê°œìˆ˜ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ë³„ íŒë§¤ëŸ‰ ì¡°íšŒ
    query = """
        SELECT DATE(r.review_date) AS date, p.category, COUNT(*) AS sales
        FROM reviews r
        JOIN products p ON r.product_id = p.product_id
        WHERE r.review_date BETWEEN '2017-06-01' AND '2025-01-31'
    """
    params = []

    # ë‚ ì§œ í•„í„° ì¶”ê°€
    if startDate and endDate:
        query += " AND r.review_date BETWEEN %s AND %s"
        params.extend([startDate, endDate])

    query += " GROUP BY DATE(r.review_date), p.category ORDER BY date"

    cursor.execute(query, params)
    results = cursor.fetchall()

    cursor.close()
    conn.close()

    # ğŸ”¹ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
    print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ íŒë§¤ëŸ‰ ë°ì´í„°: {results}")

    # ë°ì´í„° ê°€ê³µ (ë‚ ì§œë³„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘)
    category_sales_data = {}
    for row in results:
        date = row["date"].strftime("%Y-%m-%d") if row["date"] else "N/A"
        category = row["category"] if row["category"] else "ê¸°íƒ€"
        sales = row["sales"]

        if date not in category_sales_data:
            category_sales_data[date] = {}

        category_sales_data[date][category] = sales

    return category_sales_data

# âœ… ì„±ë³„ íŒë§¤ëŸ‰ API (ë¦¬ë·° ê°œìˆ˜ ê¸°ë°˜)
@app.get("/api/gender-sales")
async def gender_sales(startDate: Optional[str] = None, endDate: Optional[str] = None):
    """ì„±ë³„ íŒë§¤ëŸ‰ ì¶”ì´ ë°ì´í„° (ë¦¬ë·° ê°œìˆ˜ ê¸°ë°˜)"""
    conn = get_mysql_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")

    cursor = conn.cursor()

    # ë¦¬ë·° ê°œìˆ˜ ê¸°ë°˜ ì„±ë³„ë³„ íŒë§¤ëŸ‰ ì¡°íšŒ
    query = """
        SELECT DATE(r.review_date) AS date, p.gender, COUNT(*) AS sales
        FROM reviews r
        JOIN products p ON r.product_id = p.product_id
        WHERE r.review_date BETWEEN '2017-06-01' AND '2025-01-31'
    """
    params = []

    # ë‚ ì§œ í•„í„° ì¶”ê°€
    if startDate and endDate:
        query += " AND r.review_date BETWEEN %s AND %s"
        params.extend([startDate, endDate])

    query += " GROUP BY DATE(r.review_date), p.gender ORDER BY date"

    cursor.execute(query, params)
    results = cursor.fetchall()

    cursor.close()
    conn.close()

    # ğŸ”¹ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
    print(f"ğŸ“Š ì„±ë³„ íŒë§¤ëŸ‰ ë°ì´í„°: {results}")

    # ë°ì´í„° ê°€ê³µ (ë‚ ì§œë³„ ì„±ë³„ ë§¤í•‘)
    gender_sales_data = {}
    for row in results:
        date = row["date"].strftime("%Y-%m-%d") if row["date"] else "N/A"
        gender = row["gender"] if row["gender"] else "ê¸°íƒ€"
        sales = row["sales"]

        if date not in gender_sales_data:
            gender_sales_data[date] = {}

        gender_sales_data[date][gender] = sales

    return gender_sales_data


#--------------------------------------------------------#
#ê°•ëŒ€ì›… - ì´ìƒê°ì§€ ì‘ì—…
import time
import traceback
import asyncio
import pymysql
from datetime import datetime, date
from decimal import Decimal
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect, HTTPException, Query
from starlette.websockets import WebSocketState
from collections import defaultdict


# ğŸ”¹ Redis ì„¤ì •
REDIS_HOST = "15.152.242.221"
REDIS_PORT = 6379

# ğŸ”¹ WebSocket & ìºì‹± ì„¤ì •
ANOMALY_CACHE = None
LAST_CACHE_TIME = 0
active_connections = set()  # âœ… ë‹¨ìˆœí•œ set()ìœ¼ë¡œ ë³€ê²½
MAX_CONNECTIONS = 10
redis_client = None  # âœ… ì „ì—­ Redis ì—°ê²° ë³€ìˆ˜

async def get_redis():
    """ Redis ì—°ê²°ì„ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš© """
    global redis_client
    if redis_client is None:
        redis_client = await aioredis.from_url(f"redis://{REDIS_HOST}:{REDIS_PORT}")
    return redis_client



# âœ… WebSocket ì—°ê²° ê´€ë¦¬
async def websocket_manager():
    """ WebSocket ì—°ê²° ê°ì§€ ë° ì •ë¦¬ """
    while True:
        await asyncio.sleep(5)  # ğŸ”¥ ë¶ˆí•„ìš”í•œ ë°˜ë³µ ë°©ì§€
        disconnected_clients = set()
        for conn in active_connections.copy():
            try:
                await conn.send_json({"ping": "keep-alive"})
            except WebSocketDisconnect:
                disconnected_clients.add(conn)

        # ğŸ”¥ ì—°ê²° í•´ì œëœ í´ë¼ì´ì–¸íŠ¸ ì‚­ì œ
        for client in disconnected_clients:
            active_connections.discard(client)

        print(f"âœ… í˜„ì¬ í™œì„± WebSocket ìˆ˜: {len(active_connections)}")

async def websocket_handler(websocket: WebSocket):
    """ WebSocketì„ í†µí•´ Redis Pub/Sub ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹  ë° ì „ë‹¬ """
    await websocket.accept()
    active_connections.add(websocket)

    redis = await get_redis()
    pubsub = redis.pubsub()
    await pubsub.subscribe("review_updates")  # âœ… WebSocketì´ Redis Pub/Sub ì±„ë„ êµ¬ë…

    try:
        while True:
            message = await pubsub.get_message(ignore_subscribe_messages=True, timeout=10)
            if message and message["data"]:
                try:
                    alert_data = json.loads(message["data"].decode("utf-8"))  # âœ… JSON ë°ì´í„° ë³€í™˜
                    await websocket.send_json(alert_data)  # âœ… WebSocketìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ì— ë©”ì‹œì§€ ì „ì†¡
                    print(f"âœ… WebSocket ì•Œë¦¼ ì „ì†¡ ì„±ê³µ: {alert_data}")
                except Exception as e:
                    print(f"âŒ WebSocket ë©”ì‹œì§€ ë³€í™˜ ì˜¤ë¥˜: {e}")
            await asyncio.sleep(1)  # âœ… ë¶ˆí•„ìš”í•œ ë°˜ë³µ ìµœì†Œí™”
    except WebSocketDisconnect:
        print("ğŸ”Œ WebSocket ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤.")
    finally:
        active_connections.discard(websocket)  # âœ… ì—°ê²° ëŠê¸´ í´ë¼ì´ì–¸íŠ¸ ì œê±°
        await pubsub.unsubscribe("review_updates")  # âœ… WebSocket ì—°ê²° ì¢…ë£Œ ì‹œ Redis êµ¬ë… í•´ì œ

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    if len(active_connections) >= MAX_CONNECTIONS:
        print("âš ï¸ WebSocket ìµœëŒ€ ì—°ê²° ìˆ˜ ì´ˆê³¼. ì—°ê²° ê±°ë¶€ë¨.")
        await websocket.close()
        return

    print("ğŸ”— WebSocket í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìš”ì²­ë¨.")
    await websocket.accept()
    active_connections.add(websocket)
    print(f"âœ… WebSocket ì—°ê²°ë¨ (ì´ {len(active_connections)}ê°œ)")

    try:
        while websocket.client_state == WebSocketState.CONNECTED:
            await websocket.send_json({"ping": "keep-alive"})  
            try:
                message = await asyncio.wait_for(websocket.receive_text(), timeout=60)  # â³ 60ì´ˆë¡œ ì¦ê°€
                print(f"ğŸ“© WebSocket ë©”ì‹œì§€ ìˆ˜ì‹ : {message}")
            except asyncio.TimeoutError:
                print("â³ í´ë¼ì´ì–¸íŠ¸ ë©”ì‹œì§€ ì—†ìŒ (60ì´ˆ ê²½ê³¼)")
            await asyncio.sleep(30)  # âœ… 30ì´ˆë§ˆë‹¤ ping ë©”ì‹œì§€ ì „ì†¡
    except WebSocketDisconnect:
        print("ğŸ”Œ WebSocket ì—°ê²° í•´ì œë¨.")
    except Exception as e:
        print(f"ğŸ”¥ WebSocket ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if websocket in active_connections:
            active_connections.discard(websocket)
        print(f"âš ï¸ WebSocket í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•´ì œë¨ (ì´ {len(active_connections)}ê°œ ë‚¨ìŒ)")



# âœ… ë¶€ì • ë¦¬ë·° ê°ì§€ í›„ WebSocket ì•Œë¦¼
async def send_negative_review_alert(review):
    """ WebSocketì„ í†µí•´ ì´ìƒ ê°ì§€ ë¦¬ë·° ë°œìƒ ì•Œë¦¼ ì „ì†¡ (ì´ìƒ ê°ì§€ ìœ í˜• ì¶”ê°€) """
    redis = await get_redis()

    # âœ… ì´ë¯¸ ë³´ë‚¸ ì•Œë¦¼ì¸ì§€ Redisì—ì„œ í™•ì¸
    redis_key = f"{review['review_id']}:{review['product_id']}:{review['category']}"
    if await redis.hexists("sent_alerts", redis_key):
        print(f"âš ï¸ WebSocket ì¤‘ë³µ ë°©ì§€: ë¦¬ë·° ID {review['review_id']} ({review['category']})ëŠ” ì´ë¯¸ ì „ì†¡ë¨.")
        return  # ì´ë¯¸ ë³´ë‚¸ ì•Œë¦¼ì´ë©´ ì¢…ë£Œ

    # âœ… ì´ìƒ ê°ì§€ ìœ í˜• íŒë‹¨
    if review["sentiment"] == 0:
        anomaly_type = "ë¶€ì • ë¦¬ë·° ê°ì§€ (ëª¨ë“  í‰ì  í¬í•¨)"  
    elif review["rating"] in (1.0, 2.0, 3.0):
        anomaly_type = "í‰ì ì´ 1~3ì ì¸ ë¦¬ë·°ì…ë‹ˆë‹¤!"  
    elif review["sentiment"] == 1 and review["rating"] in (1.0, 2.0, 3.0):
        anomaly_type = "ì¤‘ë¦½ ê°ì„±ì´ì§€ë§Œ í‰ì ì´ ë‚®ìŒ!"
    else:
        print(f"â„¹ï¸ WebSocket ì•Œë¦¼ ì œì™¸ë¨: ë¦¬ë·° ID {review['review_id']} (í‰ì  {review['rating']}, ê°ì„± {review['sentiment']})")
        return  # âœ… ê°ì„± ë¶„ì„ì´ ë¶€ì •ì´ ì•„ë‹ˆê³  1~3ì ë„ ì•„ë‹Œ ê²½ìš° ì•Œë¦¼ ì œì™¸

    alert_message = {
        "alert": f"ğŸ“¢ ì´ìƒ ê°ì§€: {anomaly_type}",
        "review": {
            "review_id": review["review_id"],
            "product_id": review["product_id"],
            "product_name": review.get("product_name", "ìƒí’ˆëª… ì—†ìŒ"),
            "brand": review.get("brand", "ë¸Œëœë“œ ì—†ìŒ"),
            "price": review.get("price", "ê°€ê²© ì •ë³´ ì—†ìŒ"),
            "image_url": review.get("image_url", ""),
            "category": review.get("category", "ë¯¸ë¶„ë¥˜"),
            "sentence": review.get("sentence", "ë¦¬ë·° ë‚´ìš© ì—†ìŒ"),
            "rating": review["rating"],
            "sentiment": review["sentiment"],
            "type": anomaly_type,  
            "review_date": review.get("review_date", "ë‚ ì§œ ì—†ìŒ")
        }
    }

    # âœ… Redis Pub/Subì„ í†µí•´ WebSocketìœ¼ë¡œ ì•Œë¦¼ ë°œí–‰ (ë¡œê·¸ ì¶”ê°€)
    print(f"ğŸ“¢ [send_negative_review_alert] Redis PUBLISH ì‹¤í–‰ ì „: {alert_message}")
    await redis.publish("review_updates", json.dumps(alert_message))
    print(f"ğŸ“¢ [send_negative_review_alert] Redis PUBLISH ì‹¤í–‰ ì™„ë£Œ ğŸš€")

    await asyncio.sleep(2)  

    # âœ… WebSocket ë©”ì‹œì§€ë¥¼ ë³‘ë ¬ ì „ì†¡ (ì„±ëŠ¥ ìµœì í™”)
    disconnected_clients = set()
    send_tasks = []
    for conn in active_connections.copy():
        try:
            send_tasks.append(conn.send_json(alert_message))
        except WebSocketDisconnect:
            disconnected_clients.add(conn)
        except Exception as e:
            print(f"âš ï¸ WebSocket ë©”ì‹œì§€ ì „ì†¡ ì˜¤ë¥˜: {e}")
            disconnected_clients.add(conn)

    if send_tasks:
        await asyncio.gather(*send_tasks, return_exceptions=True)

    for client in disconnected_clients:
        active_connections.discard(client)

    print(f"âœ… WebSocketìœ¼ë¡œ {len(active_connections)}ê°œ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")

    # âœ… WebSocket ì•Œë¦¼ì´ ì •ìƒì ìœ¼ë¡œ ì „ì†¡ë˜ì—ˆìœ¼ë©´ Redisì— ê¸°ë¡
    await redis.publish("review_updates", json.dumps(alert_message))
    await redis.hset("sent_alerts", redis_key, "sent")
    await redis.expire("sent_alerts", 3600)  

    print(f"âœ… WebSocketìœ¼ë¡œ {len(active_connections)}ê°œ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")


# âœ… ìµœì‹  ë¶€ì • ë¦¬ë·° ì €ì¥ ë° ì•Œë¦¼
import json

async def save_negative_review(review):
    """ ì¤‘ë³µ ì €ì¥ì„ ë°©ì§€í•˜ê³ , ìµœëŒ€ 100ê°œì˜ ë¦¬ë·°ë§Œ ìœ ì§€ """
    redis = await get_redis()
    
    try:
        review_id = review.get("review_id")
        category = review.get("category", "ë¯¸ë¶„ë¥˜")

        # ğŸ”¹ ì¤‘ë³µ ì €ì¥ ë°©ì§€ (review_id:category í˜•ì‹ ì‚¬ìš©)
        redis_key = f"{review_id}:{category}"

        if await redis.hexists("negative_reviews_map", redis_key):
            print(f"âš ï¸ ì¤‘ë³µ ë¦¬ë·° ì €ì¥ ë°©ì§€: {redis_key}")
            return  # ì¤‘ë³µëœ ë¦¬ë·°ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (WebSocket ì•Œë¦¼ë„ X)

        json_review = json.dumps(review)

        # âœ… Redisì— ì €ì¥ (review_id:category í˜•ì‹)
        await redis.hset("negative_reviews_map", redis_key, json_review)
        await redis.lpush("negative_reviews", json_review)

        # âœ… Redis Pub/Subì— ì§ì ‘ ë©”ì‹œì§€ ë°œí–‰ (WebSocketì´ ê°ì§€í•  ìˆ˜ ìˆë„ë¡) ğŸš€
        await redis.publish("review_updates", json_review)
        print(f"ğŸ“¢ [Redis PUBLISH] ë¶€ì • ë¦¬ë·° ë©”ì‹œì§€ ì „ì†¡: {json_review}")

        # âœ… WebSocket ì•Œë¦¼ ì‹¤í–‰
        await send_negative_review_alert(review)

        # ğŸ”¹ ìµœëŒ€ ê°œìˆ˜ ìœ ì§€ (100ê°œ ì´ˆê³¼ ì‹œ ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ)
        max_reviews = 100
        current_length = await redis.llen("negative_reviews")
        if current_length > max_reviews:
            oldest_review = await redis.rpop("negative_reviews")
            if oldest_review:
                oldest_review_data = json.loads(oldest_review.decode())
                oldest_review_key = f"{oldest_review_data['review_id']}:{oldest_review_data.get('category', 'ë¯¸ë¶„ë¥˜')}"
                await redis.hdel("negative_reviews_map", oldest_review_key)  # í•´ì‹œë§µì—ì„œë„ ì‚­ì œ

    except Exception as e:
        print(f"âŒ Redis ì €ì¥ ì˜¤ë¥˜: {e}")



@app.get("/api/anomaly-products/{year}/{month}")
async def get_anomaly_products(year: int, month: int):
    """íŠ¹ì • ë‹¬ì— ì´ìƒ ê°ì§€ëœ ì œí’ˆ ì¤‘ ìƒìœ„ 10ê°œ ì¡°íšŒ"""
    start_date = f"{year}-{month:02d}-01"
    end_date = f"{year}-{month+1:02d}-01"

    query = """
        SELECT p.product_id, p.product_name, COUNT(*) AS anomaly_count
        FROM review_analysis r
        JOIN products p ON r.product_id = p.product_id
        WHERE r.review_date >= %s AND r.review_date < %s
        AND (r.sentiment = 0 OR r.rating IN (1, 2, 3))
        GROUP BY p.product_id, p.product_name
        ORDER BY anomaly_count DESC
        LIMIT 10;
    """

    try:
        pool = await get_mysql_pool()
        async with pool.acquire() as conn:
            async with conn.cursor() as cursor:
                print(f"ğŸ“¡ [API í˜¸ì¶œ] {year}-{month}ì›” ì´ìƒ ê°ì§€ ì œí’ˆ ì¡°íšŒ ì‹¤í–‰")
                await cursor.execute(query, (start_date, end_date))
                results = await cursor.fetchall()

        if not results:
            return {"error": "No Data", "message": f"{year}ë…„ {month}ì›”ì— ì´ìƒ ê°ì§€ëœ ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤."}

        return [{"product_id": row["product_id"], "product_name": row["product_name"], "anomaly_count": row["anomaly_count"]} for row in results]

    except Exception as e:
        print(f"ğŸ”¥ [API ì˜¤ë¥˜] {e}")
        return {"error": "Internal Server Error", "detail": str(e)}

@app.get("/api/negative-reviews")
async def get_negative_reviews():
    redis = await get_redis()
    reviews = await redis.lrange("negative_reviews", 0, -1)

    print(f"âœ… Redisì—ì„œ ë¶ˆëŸ¬ì˜¨ ë¶€ì • ë¦¬ë·° ê°œìˆ˜: {len(reviews)}")  # ì¶”ê°€!

    cleaned_reviews = []
    for review in reviews:
        try:
            decoded_review = review.decode()
            if not decoded_review.startswith("{"):
                decoded_review = decoded_review.replace("'", '"')
            cleaned_reviews.append(json.loads(decoded_review))
        except json.JSONDecodeError as e:
            print(f"âŒ JSON ë””ì½”ë”© ì˜¤ë¥˜: {e}")
            continue  

    return cleaned_reviews


# âœ… íŠ¹ì • ë¶€ì • ë¦¬ë·° ì‚­ì œ API
@app.delete("/api/negative-reviews/{review_id}")
async def delete_negative_review(review_id: str):
    """ Redisì—ì„œ íŠ¹ì • ë¶€ì • ë¦¬ë·° ì‚­ì œ """
    redis = await get_redis()
    reviews = await redis.lrange("negative_reviews", 0, -1)
    
    for review in reviews:
        try:
            review_data = json.loads(review.decode())
            if review_data.get("review_id") == review_id:
                await redis.lrem("negative_reviews", 1, review)
                return {"message": f"ë¶€ì • ë¦¬ë·° {review_id} ì‚­ì œ ì™„ë£Œ"}
        except json.JSONDecodeError as e:
            print(f"âŒ JSON ë””ì½”ë”© ì˜¤ë¥˜: {e}")
    
    raise HTTPException(status_code=404, detail="í•´ë‹¹ ë¶€ì • ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# âœ… JSON ë³€í™˜ í•¨ìˆ˜ (Decimal, date ë³€í™˜)
def json_serial(obj):
    """Decimal ë° date íƒ€ì…ì„ JSON ë³€í™˜ ê°€ëŠ¥í•˜ê²Œ ë³€ê²½"""
    if isinstance(obj, Decimal):
        return float(round(obj, 2))  # âœ… ì†Œìˆ˜ì  2ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼
    elif isinstance(obj, (datetime, date)):
        return obj.strftime("%Y-%m-%d %H:%M:%S")
    elif isinstance(obj, list):
        return [json_serial(item) for item in obj]
    elif isinstance(obj, dict):
        return {key: json_serial(value) for key, value in obj.items()}
    return obj



from datetime import timedelta

@app.get("/api/anomaly-summary")
async def anomaly_summary():
    """ì˜¤ëŠ˜ ë° ì´ë²ˆ ì£¼ì˜ ë¶€ì • ë¦¬ë·° ë° ì´ìƒ ê°ì§€ ë¦¬ë·° ë¶„ì„"""
    try:
        print("âœ… [anomaly_summary] API í˜¸ì¶œë¨")
        conn = get_mysql_connection()
        cursor = conn.cursor()

        today = datetime.today().strftime("%Y%m%d")
        today_table = f"today_reviews_{today}_analysis"
        print(f"ğŸ”¹ [MySQL] ì˜¤ëŠ˜ í…Œì´ë¸”: {today_table}")

        today_date = datetime.today().date()
        monday_date = today_date - timedelta(days=today_date.weekday())
        print(f"ğŸ”¹ [MySQL] ì´ë²ˆ ì£¼ ì‹œì‘ì¼: {monday_date}")

        cursor.execute(f"SHOW TABLES LIKE '{today_table}'")
        if not cursor.fetchone():
            raise HTTPException(status_code=500, detail=f"âŒ í…Œì´ë¸” {today_table}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # âœ… ì˜¤ëŠ˜ ê°ì„± ë¶„ì„
        cursor.execute(f"""
            SELECT 
                SUM(sentiment = 0) AS negative_reviews,
                SUM(sentiment = 1) AS neutral_reviews,
                SUM(sentiment = 2) AS positive_reviews
            FROM {today_table}
        """)
        row = cursor.fetchone() or {}
        today_sentiment_counts = {
            0: int(row.get("negative_reviews", 0)),
            1: int(row.get("neutral_reviews", 0)),
            2: int(row.get("positive_reviews", 0))
        }

        print(f"ğŸ“Š ì˜¤ëŠ˜ ê°ì„± ë¶„ì„ ê²°ê³¼: {today_sentiment_counts}")

        # âœ… ì˜¤ëŠ˜ í‰ì  ë¶„í¬ ë¶„ì„
        cursor.execute(f"SELECT rating, COUNT(*) AS count FROM {today_table} GROUP BY rating")
        today_rating_distribution = {
            int(row["rating"]): int(row.get("count", 0))
            for row in cursor.fetchall() if row["rating"] is not None
        }
        print(f"ğŸ“Š ì˜¤ëŠ˜ í‰ì  ë¶„í¬: {today_rating_distribution}")

        # âœ… ì˜¤ëŠ˜ í‰ê·  í‰ì  ê³„ì‚°
        cursor.execute(f"SELECT AVG(rating) AS avg_rating FROM {today_table}")
        avg_rating_result = cursor.fetchone()
        today_avg_rating = avg_rating_result["avg_rating"] if avg_rating_result and avg_rating_result["avg_rating"] else 0
        print(f"ğŸ“Œ ì˜¤ëŠ˜ í‰ê·  í‰ì : {today_avg_rating:.2f}")

        # âœ… ì˜¤ëŠ˜ ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ë·° ê°œìˆ˜ ë¶„ì„ (ë³µí•© ì¹´í…Œê³ ë¦¬ ê°œë³„ ë¶„ë¦¬)
        cursor.execute(f"""
            SELECT sub.category, COUNT(*) as count
            FROM (
                SELECT TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(category, ',', numbers.n), ',', -1)) AS category
                FROM {today_table}
                JOIN (
                    SELECT 1 as n UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4 UNION ALL SELECT 5
                ) numbers ON CHAR_LENGTH(category) - CHAR_LENGTH(REPLACE(category, ',', '')) >= numbers.n - 1
            ) AS sub
            WHERE sub.category IS NOT NULL AND sub.category <> ''
            GROUP BY sub.category;
        """)
        today_category_counts = {row["category"]: row["count"] for row in cursor.fetchall()}
        print(f"ğŸ“Š ì˜¤ëŠ˜ ì¹´í…Œê³ ë¦¬ ë¶„ì„ (ìˆ˜ì •ë¨): {today_category_counts}")

        cursor.execute(f"""
            SELECT sub.category, COUNT(*) as count
            FROM (
                SELECT TRIM(SUBSTRING_INDEX(SUBSTRING_INDEX(category, ',', numbers.n), ',', -1)) AS category
                FROM {today_table}
                JOIN (
                    SELECT 1 as n UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4 UNION ALL SELECT 5
                ) numbers ON CHAR_LENGTH(category) - CHAR_LENGTH(REPLACE(category, ',', '')) >= numbers.n - 1
                WHERE sentiment = 0  -- âœ… í•„í„°ë¥¼ ì„œë¸Œì¿¼ë¦¬ ë‚´ë¶€ê°€ ì•„ë‹Œ ì—¬ê¸°ì— ìœ„ì¹˜
            ) AS sub
            WHERE sub.category IS NOT NULL AND sub.category <> ''
            GROUP BY sub.category;
        """)
        today_negative_category_counts = {row["category"]: row["count"] for row in cursor.fetchall()}
        print(f"ğŸ“Š ì˜¤ëŠ˜ ë¶€ì • ë¦¬ë·° ì¹´í…Œê³ ë¦¬ ë¶„ì„ (ìˆ˜ì •ë¨): {today_negative_category_counts}")

        # âœ… ì˜¤ëŠ˜ ë¶€ì • ë¦¬ë·° ê°ì§€
        cursor.execute(f"""
            SELECT review_id, product_id, sentence, category, review_date, rating, sentiment
            FROM {today_table} WHERE sentiment = 0 ORDER BY review_date DESC
        """)
        today_negative_reviews = cursor.fetchall() or []
        print(f"ğŸ“Œ ì˜¤ëŠ˜ ë¶€ì • ë¦¬ë·° ê°œìˆ˜: {len(today_negative_reviews)}")

        # âœ… ì˜¤ëŠ˜ ì´ìƒ ê°ì§€ ë¦¬ë·° ê°ì§€ (ìˆ˜ì •ëœ ì¡°ê±´)
        cursor.execute(f"""
            SELECT r.review_id, r.product_id, p.product_name, p.brand, p.price, p.image_url,
                r.sentence, r.category, r.review_date, r.rating, r.sentiment,
                rv.review_size  -- âœ… ì¶”ê°€
            FROM {today_table} r
            JOIN products p ON r.product_id = p.product_id
            LEFT JOIN reviews rv ON r.review_id = rv.review_id  -- âœ… ì¶”ê°€
            WHERE r.sentiment = 0 OR r.rating IN (1.0, 2.0, 3.0)
            ORDER BY r.review_date DESC;
        """)
        today_anomaly_reviews = cursor.fetchall()
        print(f"âš ï¸ ì˜¤ëŠ˜ ì´ìƒ ê°ì§€ ë¦¬ë·° ê°œìˆ˜: {len(today_anomaly_reviews)}")


        # âœ… Redis ì €ì¥ ë° WebSocket ì•Œë¦¼ ì¶”ê°€ (ì¤‘ë³µ ê²€ì‚¬ë¥¼ ë¨¼ì € ìˆ˜í–‰)
        redis = await get_redis()  # âœ… Redis ì—°ê²° ê°€ì ¸ì˜¤ê¸°

        for review in today_anomaly_reviews:
            review_id = review["review_id"]
            review_data = {
                "review_id": review["review_id"],
                "product_id": review["product_id"],
                "sentence": review["sentence"],
                "category": review["category"],
                "review_date": json_serial(review["review_date"]),
                "rating": review["rating"],
                "sentiment": review["sentiment"],
                "review_size": review.get("review_size", "ë¯¸ì§€ì •")  # âœ… ì¶”ê°€
            }

            # âœ… ì¤‘ë³µëœ ë¦¬ë·°ì¸ì§€ Redisì—ì„œ í™•ì¸
            # âœ… ë¦¬ë·° IDì™€ ì¹´í…Œê³ ë¦¬ ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì—¬ë¶€ ì²´í¬
            redis_key = f"{review_id}:{review['category']}"  # review_id + category ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€

            if await redis.hexists("negative_reviews_map", redis_key):
                print(f"ğŸ”„ ì´ë¯¸ ì €ì¥ëœ ë¶€ì • ë¦¬ë·°: {redis_key}, WebSocket ì•Œë¦¼ ì „ì†¡ ë°©ì§€")
                continue  # ì´ë¯¸ ì €ì¥ëœ ë¦¬ë·°ëŠ” ì €ì¥ ë° ì•Œë¦¼ì„ ë³´ë‚´ì§€ ì•ŠìŒ

            # âœ… Redisì— ì €ì¥
            await redis.hset("negative_reviews_map", redis_key, json.dumps(review_data))

            print(f"âœ… Redis ì €ì¥ ì‹œë„: {review_data}")  # âœ… ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€

            try:
                # âœ… `save_negative_review()` ì‹¤í–‰ (ì¤‘ë³µ ì €ì¥ ë°©ì§€ í›„ ì €ì¥ ë° WebSocket ì•Œë¦¼)
                await save_negative_review(review_data)
            except Exception as e:
                print(f"âŒ Redis ì €ì¥ ì‹¤íŒ¨: {e}")  # âœ… Redis ì €ì¥ ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ì¶”ê°€
                traceback.print_exc()




        # âœ… ì´ë²ˆ ì£¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì´ë²ˆ ì£¼ ì›”ìš”ì¼ ~ ì˜¤ëŠ˜)
        weekly_sentiment_counts = {0: 0, 1: 0, 2: 0}
        weekly_category_counts = {}
        weekly_negative_category_counts = {}
        weekly_rating_distribution = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
        weekly_avg_rating_sum = 0
        weekly_avg_rating_count = 0
        weekly_anomaly_reviews = []  # âœ… ë£¨í”„ ë°”ê¹¥ì—ì„œ ì´ˆê¸°í™”í•˜ì—¬ ëˆ„ì  ì €ì¥ë˜ë„ë¡ ë³€ê²½

        for i in range(7):  # âœ… ì›”ìš”ì¼(03/03)ë¶€í„° ì¼ìš”ì¼(03/09)ê¹Œì§€ 7ì¼ê°„ ì¡°íšŒ
            day = monday_date + timedelta(days=i)
            day_table = f"today_reviews_{day.strftime('%Y%m%d')}_analysis"

            cursor.execute(f"SHOW TABLES LIKE '{day_table}'")
            if not cursor.fetchone():
                print(f"âš ï¸ {day_table} í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ, ê±´ë„ˆëœ€.")
                continue  # âœ… í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ í•´ë‹¹ ë‚ ì§œëŠ” ê±´ë„ˆë›°ê¸°

            print(f"âœ… [DEBUG] {day_table} í…Œì´ë¸” ì¡´ì¬ í™•ì¸ë¨.")

            # âœ… ì´ë²ˆ ì£¼ ê°ì„± ë¶„ì„
            cursor.execute(f"""
                SELECT 
                    SUM(sentiment = 0) AS negative_reviews,
                    SUM(sentiment = 1) AS neutral_reviews,
                    SUM(sentiment = 2) AS positive_reviews
                FROM {day_table}
            """)
            row = cursor.fetchone()
            weekly_sentiment_counts[0] += row["negative_reviews"] or 0
            weekly_sentiment_counts[1] += row["neutral_reviews"] or 0
            weekly_sentiment_counts[2] += row["positive_reviews"] or 0

            # âœ… ì´ë²ˆ ì£¼ í‰ì  ë¶„í¬ ë¶„ì„
            cursor.execute(f"""
                SELECT rating, COUNT(*) AS count
                FROM {day_table}
                GROUP BY rating
            """)
            for row in cursor.fetchall():
                weekly_rating_distribution[row["rating"]] += row["count"]

            # âœ… ì´ë²ˆ ì£¼ í‰ê·  í‰ì  ê³„ì‚°
            cursor.execute(f"SELECT AVG(rating) AS avg_rating FROM {day_table}")
            avg_rating = cursor.fetchone()["avg_rating"]
            if avg_rating:
                weekly_avg_rating_sum += avg_rating
                weekly_avg_rating_count += 1

            # âœ… ì´ë²ˆ ì£¼ ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ë·° ê°œìˆ˜ ë¶„ì„
            cursor.execute(f"SELECT category FROM {day_table}")
            for row in cursor.fetchall():
                categories = row.get("category", "").split(",")  # âœ… ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
                for cat in categories:
                    cat = cat.strip()
                    if cat:
                        weekly_category_counts[cat] = weekly_category_counts.get(cat, 0) + 1
            print("ğŸ“Š ì´ë²ˆ ì£¼ ì „ì²´ ì¹´í…Œê³ ë¦¬ ë°ì´í„°:", weekly_category_counts)  # âœ… ë””ë²„ê¹…

            # âœ… ì´ë²ˆ ì£¼ ë¶€ì • ë¦¬ë·° ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜ ë¶„ì„
            cursor.execute(f"SELECT category FROM {day_table} WHERE sentiment = 0")
            for row in cursor.fetchall():
                categories = row.get("category", "").split(",")  # âœ… ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
                for cat in categories:
                    cat = cat.strip()
                    if cat:
                        weekly_negative_category_counts[cat] = weekly_negative_category_counts.get(cat, 0) + 1
            print("ğŸ“Š ì´ë²ˆ ì£¼ ë¶€ì • ì¹´í…Œê³ ë¦¬ ë°ì´í„°:", weekly_negative_category_counts)  # âœ… ë””ë²„ê¹…

            # âœ… ì´ë²ˆ ì£¼ ì´ìƒ ê°ì§€ ë¦¬ë·° ëª©ë¡ (ë³€ìˆ˜ ì´ˆê¸°í™” ì œê±°)
            cursor.execute(f"""
                SELECT r.review_id, r.product_id, p.product_name, p.brand, p.price, p.image_url,
                    r.sentence, r.category, r.review_date, r.rating, r.sentiment,
                    rv.review_size  # âœ… review_size ì¶”ê°€ (JOIN)
                FROM {day_table} r
                JOIN products p ON r.product_id = p.product_id
                LEFT JOIN reviews rv ON r.review_id = rv.review_id  # âœ… reviews í…Œì´ë¸”ì—ì„œ review_size ê°€ì ¸ì˜¤ê¸°
                WHERE r.sentiment = 0  -- âœ… ëª¨ë“  ë¶€ì • ë¦¬ë·° í¬í•¨ (rating ê´€ê³„ì—†ì´)
                OR r.rating IN (1.0, 2.0, 3.0)  -- âœ… 1~3ì  ë¦¬ë·°ë„ í¬í•¨
                ORDER BY r.review_date DESC
                    """)
            weekly_anomaly_reviews.extend(cursor.fetchall())  # âœ… ë§¤ ë£¨í”„ë§ˆë‹¤ ëˆ„ì  ì €ì¥

        weekly_avg_rating = weekly_avg_rating_sum / weekly_avg_rating_count if weekly_avg_rating_count else 0

        cursor.close()
        conn.close()

        # âœ… JSON ë³€í™˜í•˜ì—¬ ë°˜í™˜
        response_data = {
            "today": {
                "sentiment_analysis": today_sentiment_counts,
                "category_analysis": today_category_counts,
                "negative_category_analysis": today_negative_category_counts,
                "negative_reviews": today_negative_reviews,
                "anomaly_reviews": today_anomaly_reviews,
                "rating_distribution": today_rating_distribution,
                "avg_rating": today_avg_rating
            },
            "weekly": {
                "sentiment_analysis": weekly_sentiment_counts,
                "category_analysis": weekly_category_counts,
                "negative_category_analysis": weekly_negative_category_counts,
                "rating_distribution": weekly_rating_distribution,
                "avg_rating": weekly_avg_rating,
                "anomaly_reviews": weekly_anomaly_reviews  # âœ… ì¶”ê°€!
            }
        }

        return json.loads(json.dumps(response_data, default=json_serial))

    except Exception as e:
        print(f"ğŸ”¥ API ì˜¤ë¥˜ ë°œìƒ: {str(e)}")  # âœ… ì¶”ê°€ ë¡œê·¸ ì¶œë ¥
        traceback.print_exc()  # âœ… ìì„¸í•œ ì˜¤ë¥˜ ì¶œë ¥
        raise HTTPException(status_code=500, detail=f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")



# âœ… Redis Pub/Sub ë¦¬ìŠ¤ë„ˆ (ë””ë²„ê¹… ì¶”ê°€)
async def redis_listener():
    redis = await get_redis()
    pubsub = redis.pubsub()
    await pubsub.subscribe("review_updates")
    print("âœ… Redis Pub/Sub 'review_updates' ì±„ë„ êµ¬ë… ì‹œì‘")

    while True:
        try:
            message = await pubsub.get_message(ignore_subscribe_messages=True, timeout=10)
            if message and message["type"] == "message":
                data = message["data"]

                try:
                    decoded_data = data.decode("utf-8")
                    parsed_data = json.loads(decoded_data)
                    print(f"ğŸ“¢ [Redis] ë©”ì‹œì§€ ìˆ˜ì‹  ì„±ê³µ: {parsed_data}")  # âœ… ì—¬ê¸°ì„œ ë©”ì‹œì§€ê°€ ë³´ì´ëŠ”ì§€ í™•ì¸!

                    # âœ… WebSocketìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡
                    disconnected_clients = set()
                    for conn in active_connections.copy():
                        try:
                            await conn.send_json(parsed_data)
                        except WebSocketDisconnect:
                            disconnected_clients.add(conn)
                        except Exception as e:
                            print(f"ğŸš¨ WebSocket ë©”ì‹œì§€ ì „ì†¡ ì˜¤ë¥˜: {e}")

                    for client in disconnected_clients:
                        active_connections.discard(client)

                except json.JSONDecodeError as json_error:
                    print(f"ğŸš¨ JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: {json_error}, ì›ë³¸ ë°ì´í„°: {data}")

            else:
                print("â³ Redis Pub/Sub ëŒ€ê¸° ì¤‘...")

            await asyncio.sleep(5)

        except Exception as e:
            print(f"âŒ Redis Pub/Sub ì˜¤ë¥˜ ë°œìƒ: {e}")

# âœ… ì¤‘ë³µ ê°ì§€ëœ ë¦¬ë·°ë¥¼ Redisì—ì„œ ì‚­ì œí•˜ëŠ” ì½”ë“œ ì¶”ê°€
async def reset_sent_alerts():
    redis = await get_redis()
    await redis.delete("sent_alerts")  # ëª¨ë“  ê¸°ì¡´ ì•Œë¦¼ ì‚­ì œ
    print("ğŸ”„ Redis `sent_alerts` ì´ˆê¸°í™” ì™„ë£Œ!")

# âœ… MySQL ë³€ê²½ ê°ì§€ í›„ Redis Pub/Sub ë©”ì‹œì§€ ì „ì†¡
# âœ… MySQL ë³€ê²½ ê°ì§€ í›„ Redis Pub/Sub ë©”ì‹œì§€ ì „ì†¡ (ìˆ˜ì •ë¨)
async def check_table_updates_task():
    """ ë§¤ì¼ ë³€ê²½ë˜ëŠ” í…Œì´ë¸”ì„ ê°ì§€í•˜ì—¬ ìƒˆë¡œìš´ ë¦¬ë·°ë§Œ ê°ì§€í•˜ê³  WebSocket ì•Œë¦¼ ì „ì†¡ """
    redis = await get_redis()

    while True:
        await asyncio.sleep(10)  # âœ… 10ì´ˆë§ˆë‹¤ ì‹¤í–‰

        today = datetime.today().strftime("%Y%m%d")
        today_table = f"today_reviews_{today}_analysis"

        conn = get_mysql_connection()
        cursor = conn.cursor()

        # ğŸ”¹ í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        cursor.execute(f"SHOW TABLES LIKE '{today_table}'")
        table_exists = cursor.fetchone()

        if not table_exists:
            print(f"â³ {today_table} í…Œì´ë¸”ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•ŠìŒ. 5ë¶„ í›„ ë‹¤ì‹œ í™•ì¸")
            cursor.close()
            conn.close()
            await asyncio.sleep(300)
            continue

        # ğŸ”¹ Redisì—ì„œ ë§ˆì§€ë§‰ ê°ì§€ëœ ë¦¬ë·° ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)
        last_review_ids = await redis.lrange("last_review_ids", 0, -1)
        last_review_ids = [id.decode() for id in last_review_ids] if last_review_ids else []

        # ğŸ”¹ ê°€ì¥ ìµœì‹  ì´ìƒ ê°ì§€ ë¦¬ë·° ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (sentiment = 0 ë˜ëŠ” rating 1~3)
        cursor.execute(f"""
            SELECT review_id, product_id, sentence, category, review_date, rating, sentiment
            FROM {today_table}
            WHERE sentiment = 0 OR rating IN (1.0, 2.0, 3.0)
            ORDER BY review_date DESC
        """)
        latest_reviews = cursor.fetchall()  # âœ… ëª¨ë“  ì´ìƒ ê°ì§€ ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°

        if not latest_reviews:
            print("â³ ê°ì§€ëœ ì´ìƒ ë¦¬ë·° ì—†ìŒ.")
            cursor.close()
            conn.close()
            continue  

        for review in latest_reviews:
            new_review_id = str(review["review_id"])

            # âœ… Redisì—ì„œ ì¤‘ë³µ ì²´í¬ ì¶”ê°€ (ì´ë¯¸ ì €ì¥ëœ ë¦¬ë·°ë©´ ë¬´ì‹œ)
            if new_review_id in last_review_ids:
                print(f"ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ëœ ë¦¬ë·°ì…ë‹ˆë‹¤: {new_review_id}")
                continue

            print(f"ğŸ“¢ ìƒˆë¡œìš´ ì´ìƒ ê°ì§€ ë¦¬ë·° ë°œê²¬! ë¦¬ë·° ID: {new_review_id}")

            review_data = {
                "review_id": review["review_id"],
                "product_id": review["product_id"],
                "sentence": review["sentence"],
                "category": review["category"],
                "review_date": json_serial(review["review_date"]),
                "rating": review["rating"],
                "sentiment": review["sentiment"]
            }

            # âœ… ì´ìƒ ê°ì§€ëœ ë¦¬ë·°ë§Œ WebSocket ì•Œë¦¼ ì „ì†¡
            print(f"âš ï¸ ì´ìƒ ê°ì§€ ë¦¬ë·° ì•Œë¦¼ ë°œì†¡: {review_data}")
            await send_negative_review_alert(review_data)

            # ğŸ”¹ Redisì— ìµœì‹  ë¦¬ë·° ID ì €ì¥ (ì¤‘ë³µ ê°ì§€ ë°©ì§€, ì—¬ëŸ¬ ê°œ ê´€ë¦¬)
            await redis.lpush("last_review_ids", new_review_id)
            await redis.ltrim("last_review_ids", 0, 99)  # âœ… ìµœëŒ€ 100ê°œë§Œ ì €ì¥ (ì´ì „ ë¦¬ë·° ID ê´€ë¦¬)

        cursor.close()
        conn.close()

# âœ… MySQL ë¹„ë™ê¸° ì—°ê²° í’€ ìƒì„±
async def get_mysql_pool():
    return await aiomysql.create_pool(
        host="15.152.242.221",
        user="admin",
        password="Admin@1234",
        db="musinsa_pd_rv",
        cursorclass=aiomysql.DictCursor,
        autocommit=True
    )

@app.get("/api/anomaly-shoes-category/{year}/{month}")
async def get_anomaly_shoes_category(year: int, month: int):
    """ì‹ ë°œ ì¹´í…Œê³ ë¦¬ë³„ ì´ìƒ ê°ì§€ ë¦¬ë·° ë°œìƒ ë¹ˆë„ ì¡°íšŒ"""
    try:
        # âœ… ì‹œì‘ ë‚ ì§œ: í•´ë‹¹ ì›” 1ì¼
        # response = await some_function_to_fetch_data(year, month)
        # headers = {"Cache-Control": "max-age=600"}  # 10ë¶„ ë™ì•ˆ ìºì‹œ ìœ ì§€
        # return JSONResponse(content=response, headers=headers)
        start_date = f"{year}-{month:02d}-01"
        end_date = f"{year}-{month+1:02d}-01"

        # âœ… ì‹ ë°œ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ëª©ë¡
        shoe_categories = ["êµ¬ë‘", "ë“±ì‚°í™”", "ë¶€ì¸ /ì›Œì»¤", "ìƒŒë“¤/ìŠ¬ë¦¬í¼",
                           "ìŠ¤í¬ì¸ í™”", "ìº”ë²„ìŠ¤/ë‹¨í™”", "íŠ¸ë˜í‚¹í™”", "í"]

        # âœ… SQLì—ì„œ ì¹´í…Œê³ ë¦¬ í•„í„°ë¥¼ ì§ì ‘ ì ìš©
        category_filter = ", ".join(f"'{cat}'" for cat in shoe_categories)

        query = f"""
            SELECT p.category, COUNT(*) AS anomaly_count
            FROM review_analysis r
            JOIN products p ON r.product_id = p.product_id
            WHERE r.review_date >= %s AND r.review_date < %s  -- âœ… ë‹¤ìŒ ë‹¬ 1ì¼ ë¯¸í¬í•¨
            AND (r.sentiment = 0 OR r.rating IN (1, 2, 3))
            AND p.category IN ({category_filter})  -- âœ… ì‹ ë°œ ì¹´í…Œê³ ë¦¬ í•„í„°ë§
            GROUP BY p.category
            ORDER BY anomaly_count DESC;
        """

        async with await get_mysql_pool() as pool:
            async with pool.acquire() as conn:
                async with conn.cursor(aiomysql.DictCursor) as cursor:
                    await cursor.execute(query, (start_date, end_date))  # âœ… start_date, end_dateë§Œ ë°”ì¸ë”©
                    results = await cursor.fetchall()

        print(f"ğŸ” SQL ì¡°íšŒ ê²°ê³¼: {results}")  # âœ… FastAPI ë¡œê·¸ì—ì„œ ë°ì´í„° í™•ì¸

        if not results:
            return {"error": "No Data", "message": f"{year}ë…„ {month}ì›” ì‹ ë°œ ì¹´í…Œê³ ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        return {"data": [{"category": row["category"], "anomaly_count": row["anomaly_count"]} for row in results]}

    except Exception as e:
        import traceback
        error_message = traceback.format_exc()  # ì˜ˆì™¸ ë°œìƒ ìœ„ì¹˜ê¹Œì§€ í¬í•¨í•˜ì—¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        print(f"ğŸ”¥ [API ì˜¤ë¥˜ ë°œìƒ]: {error_message}")  # ì½˜ì†” ë¡œê·¸ ì¶œë ¥
        return {"error": "Internal Server Error", "detail": error_message}  # API ì‘ë‹µì— ì˜¤ë¥˜ ìƒì„¸ ì •ë³´ í¬í•¨





@app.get("/api/anomaly-trend/{year}/{month}")
async def get_anomaly_trend(year: int, month: int):
    start_date = f"{year}-{month:02d}-01"

    query = """
        SELECT DATE(review_date) AS review_date,  
            SUM(sentiment = 0) AS negative_count,
            SUM(rating IN (1.0, 2.0, 3.0)) AS low_rating_count
        FROM review_analysis
        WHERE DATE(review_date) BETWEEN %s AND LAST_DAY(%s)
        AND DATE(review_date) != '2025-03-04'  -- âœ… íŠ¹ì • ë‚ ì§œ ì œì™¸
        GROUP BY DATE(review_date)
        ORDER BY review_date;
    """

    try:
        pool = await get_mysql_pool()
        async with pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(query, (start_date, start_date))
                results = await cursor.fetchall()

        if not results:
            return {"error": "No Data", "message": f"{year}ë…„ {month}ì›” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        return [
            {
                "date": row["review_date"].strftime("%Y-%m-%d"),
                "negative_count": row["negative_count"],
                "low_rating_count": row["low_rating_count"]
            }
            for row in results
        ]

    except Exception as e:
        print(f"ğŸ”¥ API ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": "Internal Server Error", "detail": str(e)}





async def check_redis_negative_reviews():
    redis = await aioredis.from_url("redis://15.152.242.221:6379")
    
    # ì €ì¥ëœ ë¶€ì • ë¦¬ë·° í•´ì‹œë§µ í™•ì¸
    negative_reviews = await redis.hgetall("negative_reviews_map")
    
    # ì¶œë ¥
    print("ğŸ“Œ Redisì— ì €ì¥ëœ ë¶€ì • ë¦¬ë·°:")
    for key, value in negative_reviews.items():
        try:
            review = json.loads(value.decode()) if value else None
            if review:
                print(f"ğŸ”¹ ë¦¬ë·° ID: {review['review_id']}, í‰ì : {review['rating']}, ê°ì„±: {review['sentiment']}, ë¬¸ì¥: {review['sentence']}")
        except Exception as e:
            print(f"âŒ JSON ë””ì½”ë”© ì˜¤ë¥˜: {e}")

async def main():
    await check_redis_negative_reviews()

# if __name__ == "__main__":
#     asyncio.run(main())  # âœ… FastAPI ì‹¤í–‰ ì „ì— ì‹¤í–‰í•  ê²½ìš°ì—ë§Œ ì‚¬ìš©

from collections import Counter
from fastapi import HTTPException
from openai import AsyncOpenAI

@app.get("/api/anomaly-guideline/{product_id}")
async def generate_guideline_for_product(product_id: int):
    redis = await get_redis()
    cache_key = f"guideline:product:{product_id}"

    cached = await redis.get(cache_key)
    if cached:
        return {"guideline": cached}

    try:
        summary = await anomaly_summary()
        reviews = summary.get("weekly", {}).get("anomaly_reviews", [])
        product_reviews = [
            r for r in reviews
            if str(r.get("product_id")) == str(product_id)
        ]
    except Exception:
        raise HTTPException(status_code=500, detail="anomaly-summary í˜¸ì¶œ ì‹¤íŒ¨")

    if not product_reviews:
        return {"guideline": f"ID {product_id}ì— ëŒ€í•œ ì´ìƒ ë¦¬ë·°ê°€ ì—†ì–´ ê°€ì´ë“œë¼ì¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    # âœ… ê°ì„± ë¼ë²¨ ë³€í™˜ í•¨ìˆ˜
    def get_sentiment_label(code):
        return {0: "ë¶€ì •", 1: "ì¤‘ë¦½", 2: "ê¸ì •"}.get(code, "ì•Œ ìˆ˜ ì—†ìŒ")

    # âœ… ê°ì„± í†µê³„ ìš”ì•½
    sentiment_counter = Counter(r['sentiment'] for r in product_reviews)
    sentiment_summary = f"ë¶€ì •: {sentiment_counter.get(0, 0)}ê°œ, ì¤‘ë¦½: {sentiment_counter.get(1, 0)}ê°œ, ê¸ì •: {sentiment_counter.get(2, 0)}ê°œ"

    # âœ… í‰ê·  í‰ì  ê³„ì‚°
    avg_rating = sum(r["rating"] for r in product_reviews if r.get("rating")) / len(product_reviews)

    # âœ… ì£¼ìš” ì¹´í…Œê³ ë¦¬ ë¶„ì„
    category_counter = Counter(
        c.strip() for r in product_reviews for c in r.get("category", "").split(",") if c.strip()
    )
    top_categories = ", ".join(f"{cat}({cnt}íšŒ)" for cat, cnt in category_counter.most_common(5))

    # âœ… ë¦¬ë·° ìƒ˜í”Œ ëª¨ë‘ í¬í•¨ (ë„ˆë¬´ ë§ì„ ê²½ìš° [:30]ë¡œ ì œí•œí•´ë„ ë¨)
    sample_reviews = "\n".join([
        f'- "{r["sentence"]}" (í‰ì : {r["rating"]}, ê°ì„±: {get_sentiment_label(r["sentiment"])})'
        for r in product_reviews
    ])
    product_name = product_reviews[0].get("product_name", "ì œí’ˆëª… ì—†ìŒ")  # ì²« ë¦¬ë·°ì—ì„œ ì¶”ì¶œ
    # âœ… GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„± (â—ì¤‘ìš”: í‰ì  â‰  ê°ì„± ì§€ì‹œ í¬í•¨)
    prompt = f"""
    [ìƒí’ˆ ì •ë³´]
ğŸ“ [{product_name}] â† ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ í¬í•¨í•´ì¤˜ì•¼ í•´
ì´ìƒ ê°ì§€ ë¦¬ë·° ë¶„ì„ - ìƒí’ˆ ID {product_id}
- ë¦¬ë·° ìˆ˜: {len(product_reviews)}
- í‰ê·  í‰ì : {avg_rating:.2f}
- ê°ì„± ë¶„ì„ ìš”ì•½: {sentiment_summary}
- ì£¼ìš” ê°ì„±/í† í”½: {top_categories}

âš ï¸ ë¦¬ë·° í‰ì ì´ ë†’ë”ë¼ë„, ê°ì„± ë¶„ì„ ê²°ê³¼ê°€ 'ë¶€ì •'ì´ë©´ ë°˜ë“œì‹œ ë¬¸ì œë¡œ ê°„ì£¼í•´ì•¼ í•©ë‹ˆë‹¤.
GPTëŠ” ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ì‹ ë¢°í•˜ì—¬ ë¦¬ë·° í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ë¶„ì„í•˜ì„¸ìš”.

[ì´ìƒ ê°ì§€ ë¦¬ë·° ëª©ë¡]
{sample_reviews}

ìœ„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•œ **ì œí’ˆ ê°œì„  ê°€ì´ë“œë¼ì¸**ì„ ì‘ì„±í•´ì¤˜:
1. ë¦¬ë·°ì™€ í‰ì , ê°ì„± ë¶„ì„ ë“±ì„ í†µí•œ ë¬¸ì œ ì›ì¸ ë¶„ì„
2. ìì£¼ ì–¸ê¸‰ëœ ë¬¸ì œ í† í”½ ê¸°ë°˜ ìš”ì•½
3. ê³ ê° ì´íƒˆ ë°©ì§€ë¥¼ ìœ„í•œ ì¢…í•©ì ì¸ ê°œì„  ì „ëµ ì œì‹œ

ê° í•­ëª©ì€ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¼ì¤˜:
1. ë¬¸ì œ ì›ì¸ ë¶„ì„: (ì†Œì œëª© í˜•íƒœë¡œ ì‹œì‘í•˜ê³ , ë‹¨ë½ìœ¼ë¡œ ì„œìˆ )
2. ë¬¸ì œ í† í”½ ìš”ì•½: (ì£¼ìš” í‚¤ì›Œë“œë¥¼ ê°•ì¡°í•˜ë©° ë¬¸ì¥í™”)
3. ê°œì„  ì „ëµ ì œì‹œ: (a, b, c í•­ëª©ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆ)

ê° ì„¹ì…˜ì€ ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ ë˜ëŠ” HTML ì—†ì´ í‰ë²”í•œ í…ìŠ¤íŠ¸ë¡œ, ê¹”ë”í•˜ê²Œ ë‹¨ë½ ìœ„ì£¼ë¡œ ì‘ì„±í•´ì¤˜.
"""

    # âœ… GPT í˜¸ì¶œ
    client = AsyncOpenAI(api_key="OPENAI_API_KEY")
    try:
        response = await client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì œí’ˆ ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì•¼."},
                {"role": "user", "content": prompt}
            ]
        )
        result = response.choices[0].message.content
        await redis.set(cache_key, result, ex=86400)
        return {"guideline": result}
    except Exception:
        raise HTTPException(status_code=500, detail="GPT í˜¸ì¶œ ì‹¤íŒ¨")
# ########
@app.get("/api/anomaly-guideline-today/{product_id}")
async def generate_guideline_today(product_id: int):
    redis = await get_redis()
    cache_key = f"guideline:product:{product_id}:today"

    cached = await redis.get(cache_key)
    if cached:
        return {"guideline": cached}

    try:
        summary = await anomaly_summary()
        today_reviews = summary.get("today", {}).get("anomaly_reviews", [])
        product_reviews = [
            r for r in today_reviews
            if str(r.get("product_id")) == str(product_id)
        ]
    except Exception:
        raise HTTPException(status_code=500, detail="anomaly-summary í˜¸ì¶œ ì‹¤íŒ¨")

    if not product_reviews:
        return {"guideline": f"ID {product_id}ì— ëŒ€í•œ ì˜¤ëŠ˜ ë¦¬ë·°ê°€ ì—†ì–´ ê°€ì´ë“œë¼ì¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    def get_sentiment_label(code):
        return {0: "ë¶€ì •", 1: "ì¤‘ë¦½", 2: "ê¸ì •"}.get(code, "ì•Œ ìˆ˜ ì—†ìŒ")

    sentiment_counter = Counter(r['sentiment'] for r in product_reviews)
    sentiment_summary = f"ë¶€ì •: {sentiment_counter.get(0, 0)}ê°œ, ì¤‘ë¦½: {sentiment_counter.get(1, 0)}ê°œ, ê¸ì •: {sentiment_counter.get(2, 0)}ê°œ"

    avg_rating = sum(r["rating"] for r in product_reviews if r.get("rating")) / len(product_reviews)

    category_counter = Counter(
        c.strip() for r in product_reviews for c in r.get("category", "").split(",") if c.strip()
    )
    top_categories = ", ".join(f"{cat}({cnt}íšŒ)" for cat, cnt in category_counter.most_common(5))

    sample_reviews = "\n".join([
        f'- "{r["sentence"]}" (í‰ì : {r["rating"]}, ê°ì„±: {get_sentiment_label(r["sentiment"])})'
        for r in product_reviews
    ])
    product_name = product_reviews[0].get("product_name", "ì œí’ˆëª… ì—†ìŒ")  # ì²« ë¦¬ë·°ì—ì„œ ì¶”ì¶œ
    prompt = f"""
    [ìƒí’ˆ ì •ë³´]
ğŸ“ [{product_name}] â† ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ í¬í•¨í•´ì¤˜ì•¼ í•´
[ì˜¤ëŠ˜ ë¦¬ë·° ê¸°ë°˜ ë¶„ì„] - ìƒí’ˆ ID {product_id}
- ë¦¬ë·° ìˆ˜: {len(product_reviews)}
- í‰ê·  í‰ì : {avg_rating:.2f}
- ê°ì„± ë¶„ì„ ìš”ì•½: {sentiment_summary}
- ì£¼ìš” ê°ì„±/í† í”½: {top_categories}

âš ï¸ ë¦¬ë·° í‰ì ì´ ë†’ë”ë¼ë„, ê°ì„± ë¶„ì„ ê²°ê³¼ê°€ 'ë¶€ì •'ì´ë©´ ë°˜ë“œì‹œ ë¬¸ì œë¡œ ê°„ì£¼í•´ì•¼ í•©ë‹ˆë‹¤.
GPTëŠ” ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ì‹ ë¢°í•˜ì—¬ ë¦¬ë·° í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ë¶„ì„í•˜ì„¸ìš”.

[ì´ìƒ ê°ì§€ ë¦¬ë·° ëª©ë¡]
{sample_reviews}

ìœ„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•œ **ì œí’ˆ ê°œì„  ê°€ì´ë“œë¼ì¸**ì„ ì‘ì„±í•´ì¤˜:
1. ë¦¬ë·°ì™€ í‰ì , ê°ì„± ë¶„ì„ ë“±ì„ í†µí•œ ë¬¸ì œ ì›ì¸ ë¶„ì„
2. ìì£¼ ì–¸ê¸‰ëœ ë¬¸ì œ í† í”½ ê¸°ë°˜ ìš”ì•½
3. ê³ ê° ì´íƒˆ ë°©ì§€ë¥¼ ìœ„í•œ ì¢…í•©ì ì¸ ê°œì„  ì „ëµ ì œì‹œ

ê° í•­ëª©ì€ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¼ì¤˜:
1. ë¬¸ì œ ì›ì¸ ë¶„ì„: (ì†Œì œëª© í˜•íƒœë¡œ ì‹œì‘í•˜ê³ , ë‹¨ë½ìœ¼ë¡œ ì„œìˆ )
2. ë¬¸ì œ í† í”½ ìš”ì•½: (ì£¼ìš” í‚¤ì›Œë“œë¥¼ ê°•ì¡°í•˜ë©° ë¬¸ì¥í™”)
3. ê°œì„  ì „ëµ ì œì‹œ: (a, b, c í•­ëª©ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆ)

ê° ì„¹ì…˜ì€ ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ ë˜ëŠ” HTML ì—†ì´ í‰ë²”í•œ í…ìŠ¤íŠ¸ë¡œ, ê¹”ë”í•˜ê²Œ ë‹¨ë½ ìœ„ì£¼ë¡œ ì‘ì„±í•´ì¤˜.
"""

    client = AsyncOpenAI(api_key="OPENAI_API_KEY")
    try:
        response = await client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì œí’ˆ ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì•¼."},
                {"role": "user", "content": prompt}
            ]
        )
        result = response.choices[0].message.content
        await redis.set(cache_key, result, ex=3600)  # 1ì‹œê°„ ìºì‹œ
        return {"guideline": result}
    except Exception:
        raise HTTPException(status_code=500, detail="GPT í˜¸ì¶œ ì‹¤íŒ¨")

import os  # âœ… ì´ ì¤„ ì¶”ê°€

@app.get("/api/anomaly-guideline-monthly/{year}/{month}")
async def generate_monthly_guideline(year: int, month: int):
    redis = await get_redis()
    cache_key = f"guideline:monthly:{year}-{month:02d}"

    cached = await redis.get(cache_key)
    if cached:
        return {"guideline": cached}

    try:
        # ğŸ”¹ ê¸°ì¡´ 3ê°œ API í˜¸ì¶œ
        trend_task = get_anomaly_trend(year, month)
        category_task = get_anomaly_shoes_category(year, month)
        product_task = get_anomaly_products(year, month)

        trend, category, products = await asyncio.gather(trend_task, category_task, product_task)

        # ğŸ”¸ ë°ì´í„° ì •ë¦¬
        trend_text = "\n".join([
            f"- {row['date']}: ë¶€ì • ë¦¬ë·° {row['negative_count']}ê±´, ì €í‰ì  {row['low_rating_count']}ê±´"
            for row in trend
        ])

        category_data = category.get("data", [])
        category_text = "\n".join([
            f"- {row['category']}: {row['anomaly_count']}ê±´"
            for row in category_data
        ])

        product_text = "\n".join([
            f"- {row['product_name']} (ID: {row['product_id']}): {row['anomaly_count']}ê±´"
            for row in products
        ])

        # ğŸ”¸ GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
[2025ë…„ {month}ì›”] ì´ìƒ ê°ì§€ ë¦¬ë·° í†µê³„ ë¶„ì„

1. ğŸ“ˆ ì´ìƒ ê°ì§€ ì¶”ì´
{trend_text}

2. ğŸ“¦ ì¹´í…Œê³ ë¦¬ë³„ ì´ìƒ ê°ì§€ ë¶„í¬
{category_text}

3. ğŸ” ì´ìƒ ê°ì§€ ì œí’ˆ Top 10
{product_text}

ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ í•­ëª©ì„ í¬í•¨í•œ ì›”ê°„ ì¢…í•© ê°œì„  ê°€ì´ë“œë¼ì¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

- ì „ë°˜ì ì¸ ì´ìƒ ê°ì§€ íŠ¸ë Œë“œ ìš”ì•½
- ì£¼ì˜ê°€ í•„ìš”í•œ ì œí’ˆêµ° ë¶„ì„
- ì£¼ìš” ì œí’ˆì— ëŒ€í•œ ë¬¸ì œ ìš”ì•½ ë° ê°œì„  ë°©í–¥
- ë¦¬ë·° ê°ì„± ë° í‰ì  ë¶„í¬ì˜ ì°¨ì´ì— ëŒ€í•œ ì‹œì‚¬ì 

ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ, ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•´ì£¼ì„¸ìš”.
"""

        # ğŸ”¹ GPT í˜¸ì¶œ
        client = AsyncOpenAI(api_key="OPENAI_API_KEY")
        response = await client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì•¼. ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì¤˜."},
                {"role": "user", "content": prompt}
            ]
        )

        guideline = response.choices[0].message.content
        await redis.set(cache_key, guideline, ex=86400)

        return {"guideline": guideline}

    except Exception as e:
        import traceback
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


# @app.delete("/api/anomaly-guideline/{product_id}/cache")
# async def delete_guideline_cache(product_id: int):
#     redis = await get_redis()
#     cache_key = f"guideline:product:{product_id}"
#     deleted = await redis.delete(cache_key)
#     return {
#         "cache_key": cache_key,
#         "deleted": bool(deleted),
#         "message": "ìºì‹œ ì‚­ì œ ì™„ë£Œ" if deleted else "ìºì‹œì— í•´ë‹¹ í‚¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
#     }





@app.get("/")
async def home(request: Request):
    """ ê¸°ë³¸ ê²½ë¡œì—ì„œ index.htmlì„ ë Œë”ë§í•˜ê³ , ì´ìƒ ê°ì§€ ë¦¬ë·° ë°ì´í„°ë¥¼ ì „ë‹¬ """
    redis = await get_redis()
    reviews = await redis.lrange("negative_reviews", 0, -1)

    try:
        negative_reviews = [json.loads(review.decode()) for review in reviews] if reviews else []
    except json.JSONDecodeError as e:
        print(f"âŒ JSON ë””ì½”ë”© ì˜¤ë¥˜: {e}")
        negative_reviews = []

    return templates.TemplateResponse("index.html", {
        "request": request,
        "today_date": datetime.today().strftime("%Y-%m-%d"),
        "negative_reviews": negative_reviews
    })

# # ğŸ”¹ ì´ìƒ ê°ì§€ í˜ì´ì§€ ë Œë”ë§
# @app.get("/anomaly-detection.html")
# async def anomaly_detection_page(request: Request):
#     redis = await get_redis()
#     reviews = await redis.lrange("negative_reviews", 0, -1)

#     # âœ… ë°ì´í„° ë””ì½”ë”© ê³¼ì • ê°œì„  (json.loads() ì‚¬ìš©)
#     try:
#         negative_reviews = [json.loads(review.decode()) for review in reviews] if reviews else []
#     except json.JSONDecodeError as e:
#         print(f"âŒ JSON ë””ì½”ë”© ì˜¤ë¥˜: {e}")
#         negative_reviews = []

#     return templates.TemplateResponse("anomaly-detection.html", {
#         "request": request,
#         "today_date": datetime.today().strftime("%Y-%m-%d"),
#         "negative_reviews": negative_reviews
#     })

# âœ… ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹¤í–‰
@app.on_event("startup")
async def startup_event():
    try:
        """ ì„œë²„ ì‹œì‘ ì‹œ ìë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì•Œë¦¼ì„ ë°œí–‰ (1íšŒ ì‹¤í–‰) """
        print("ğŸ”¹ ì„œë²„ ì‹œì‘ë¨. Redis í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ë°œí–‰!")
        asyncio.ensure_future(redis_listener())  # ğŸ”¥ Redis Pub/Sub ë¦¬ìŠ¤ë„ˆ ì‹¤í–‰
        asyncio.ensure_future(check_table_updates_task())  # ğŸ”¥ MySQL ë³€ê²½ ê°ì§€ íƒœìŠ¤í¬ ì‹¤í–‰
        # âœ… Redisì— ì €ì¥ëœ ë¶€ì • ë¦¬ë·° í™•ì¸ (ì´ì „ ë¦¬ë·° ë¡œë“œ)
        asyncio.ensure_future(check_redis_negative_reviews())
    except Exception as e:
        print(f"ğŸš¨ Redis ë° MySQL ë³€ê²½ ê°ì§€ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì‹¤íŒ¨: {e}")
#--------------------------------------------------------#